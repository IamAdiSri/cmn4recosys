{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Collaborative Memory Network for Recommendation Systems\n",
    "**_Ebesu, Shen, Fang - The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval - SIGIR '18_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://github.com/IamAdiSri/cmn4recosys) notebook by [**Aditya Srivastava**](https://github.com/IamAdiSri/) is a PyTorch port to the original TensorFlow [project](https://github.com/tebesu/CollaborativeMemoryNetwork)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:11.706288Z",
     "start_time": "2019-07-09T21:39:11.229005Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:12.161578Z",
     "start_time": "2019-07-09T21:39:11.822450Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:13.194020Z",
     "start_time": "2019-07-09T21:39:13.185834Z"
    }
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    resume = False\n",
    "    ssdir = 'snapshots/'\n",
    "    logdir = 'logs/'\n",
    "    version = 'model_1_scheduler'\n",
    "    dataset = 'data/citeulike-a.npz'\n",
    "    pretrain = 'pretrain/citeulike-a_e50.npz'\n",
    "    embed_size = 50\n",
    "    epochs = 30 # training epochs (originally 30)\n",
    "    batch_size = 128\n",
    "    hops = 2 # number of hops/layers\n",
    "    l2_lambda = 0.1 # l2 regularization\n",
    "    neg_count = 4 # negative samples count\n",
    "    optimizer = 'rmsprop'\n",
    "    learning_rate = 0.001\n",
    "    decay_rate = 0.9\n",
    "    momentum = 0.9\n",
    "    grad_clip = 5.0\n",
    "    tol = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:14.006921Z",
     "start_time": "2019-07-09T21:39:13.986837Z"
    },
    "code_folding": [
     0
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:18.109415Z",
     "start_time": "2019-07-09T21:39:18.075932Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "import smtplib\n",
    "def send_mail(notification):\n",
    "    \"\"\"\n",
    "    sends an smtp email notification\n",
    "    \"\"\"\n",
    "    with open('.email_config', 'r') as f:\n",
    "        c = f.read().split(' ')\n",
    "        \n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "\n",
    "        #Next, log in to the server\n",
    "        server.login(c[0], c[1])\n",
    "\n",
    "        #Send the mail\n",
    "        msg = \"\\n\" + notification # The /n separates the message from the headers\n",
    "        server.sendmail(c[2], c[3], msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:19.336369Z",
     "start_time": "2019-07-09T21:39:19.296463Z"
    },
    "code_folding": [
     2,
     27,
     35,
     42,
     48,
     54,
     72,
     83
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"\n",
    "        Wraps dataset and produces batches for the model to consume\n",
    "\n",
    "        :param filename: path to training data for npz file\n",
    "        \"\"\"\n",
    "        self._data = np.load(filename, allow_pickle=True)\n",
    "        self.train_data = self._data['train_data'][:, :2]\n",
    "        self.test_data = self._data['test_data'].tolist()\n",
    "        self._train_index = np.arange(len(self.train_data), dtype=np.uint)\n",
    "        self._n_users, self._n_items = self.train_data.max(axis=0) + 1\n",
    "\n",
    "        # Neighborhoods\n",
    "        self.user_items = defaultdict(set)\n",
    "        self.item_users = defaultdict(set)\n",
    "        for u, i in self.train_data:\n",
    "            self.user_items[u].add(i)\n",
    "            self.item_users[i].add(u)\n",
    "        # Get a list version so we do not need to perform type casting\n",
    "        self.item_users_list = {k: list(v) for k, v in self.item_users.items()}\n",
    "        self._max_user_neighbors = max([len(x) for x in self.item_users.values()])\n",
    "        self.user_items = dict(self.user_items)\n",
    "        self.item_users = dict(self.item_users)\n",
    "\n",
    "    @property\n",
    "    def train_size(self):\n",
    "        \"\"\"\n",
    "        :return: number of examples in training set\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return len(self.train_data)\n",
    "\n",
    "    @property\n",
    "    def user_count(self):\n",
    "        \"\"\"\n",
    "        Number of users in dataset\n",
    "        \"\"\"\n",
    "        return self._n_users\n",
    "\n",
    "    @property\n",
    "    def item_count(self):\n",
    "        \"\"\"\n",
    "        Number of items in dataset\n",
    "        \"\"\"\n",
    "        return self._n_items\n",
    "\n",
    "    def _sample_item(self):\n",
    "        \"\"\"\n",
    "        Draw an item uniformly\n",
    "        \"\"\"\n",
    "        return np.random.randint(0, self.item_count)\n",
    "\n",
    "    def _sample_negative_item(self, user_id):\n",
    "        \"\"\"\n",
    "        Uniformly sample a negative item\n",
    "        \"\"\"\n",
    "        if user_id > self.user_count:\n",
    "            raise ValueError(\"Trying to sample user id: {} > user count: {}\".format(\n",
    "                user_id, self.user_count))\n",
    "\n",
    "        n = self._sample_item()\n",
    "        positive_items = self.user_items[user_id]\n",
    "\n",
    "        if len(positive_items) >= self.item_count:\n",
    "            raise ValueError(\"The User has rated more items than possible %s / %s\" % (\n",
    "                len(positive_items), self.item_count))\n",
    "        while n in positive_items or n not in self.item_users:\n",
    "            n = self._sample_item()\n",
    "        return n\n",
    "\n",
    "    def _generate_data(self, neg_count):\n",
    "        idx = 0\n",
    "        self._examples = np.zeros((self.train_size*neg_count, 3),\n",
    "                                  dtype=np.uint32)\n",
    "        self._examples[:, :] = 0\n",
    "        for user_idx, item_idx in self.train_data:\n",
    "            for _ in range(neg_count):\n",
    "                neg_item_idx = self._sample_negative_item(user_idx)\n",
    "                self._examples[idx, :] = [user_idx, item_idx, neg_item_idx]\n",
    "                idx += 1\n",
    "\n",
    "    def get_data(self, batch_size: int, neighborhood: bool, neg_count: int):\n",
    "        \"\"\"\n",
    "        Batch data together as (user, item, negative item), pos_neighborhood,\n",
    "        length of neighborhood, negative_neighborhood, length of negative neighborhood\n",
    "\n",
    "        if neighborhood is False returns only user, item, negative_item so we\n",
    "        can reuse this for non-neighborhood-based methods.\n",
    "\n",
    "        :param batch_size: size of the batch\n",
    "        :param neighborhood: return the neighborhood information or not\n",
    "        :param neg_count: number of negative samples to uniformly draw per a pos\n",
    "                          example\n",
    "        :return: generator\n",
    "        \"\"\"\n",
    "        # Allocate inputs\n",
    "        batch = np.zeros((batch_size, 3), dtype=np.uint32)\n",
    "        pos_neighbor = np.zeros((batch_size, self._max_user_neighbors), dtype=np.int32)\n",
    "        pos_length = np.zeros(batch_size, dtype=np.int32)\n",
    "        neg_neighbor = np.zeros((batch_size, self._max_user_neighbors), dtype=np.int32)\n",
    "        neg_length = np.zeros(batch_size, dtype=np.int32)\n",
    "\n",
    "        # Shuffle index\n",
    "        np.random.shuffle(self._train_index)\n",
    "\n",
    "        idx = 0\n",
    "        for user_idx, item_idx in self.train_data[self._train_index]:\n",
    "            # TODO: set positive values outside of for loop\n",
    "            for _ in range(neg_count):\n",
    "                neg_item_idx = self._sample_negative_item(user_idx)\n",
    "                batch[idx, :] = [user_idx, item_idx, neg_item_idx]\n",
    "\n",
    "                # Get neighborhood information\n",
    "                if neighborhood:\n",
    "                    if len(self.item_users.get(item_idx, [])) > 0:\n",
    "                        pos_length[idx] = len(self.item_users[item_idx])\n",
    "                        pos_neighbor[idx, :pos_length[idx]] = self.item_users_list[item_idx]\n",
    "                    else:\n",
    "                        # Length defaults to 1\n",
    "                        pos_length[idx] = 1\n",
    "                        pos_neighbor[idx, 0] = item_idx\n",
    "\n",
    "                    if len(self.item_users.get(neg_item_idx, [])) > 0:\n",
    "                        neg_length[idx] = len(self.item_users[neg_item_idx])\n",
    "                        neg_neighbor[idx, :neg_length[idx]] = self.item_users_list[neg_item_idx]\n",
    "                    else:\n",
    "                        # Length defaults to 1\n",
    "                        neg_length[idx] = 1\n",
    "                        neg_neighbor[idx, 0] = neg_item_idx\n",
    "\n",
    "                idx += 1\n",
    "                # Yield batch if we filled queue\n",
    "                if idx == batch_size:\n",
    "                    if neighborhood:\n",
    "                        max_length = max(neg_length.max(), pos_length.max())\n",
    "                        yield batch, pos_neighbor[:, :max_length], pos_length, \\\n",
    "                              neg_neighbor[:, :max_length], neg_length\n",
    "                        pos_length[:] = 1\n",
    "                        neg_length[:] = 1\n",
    "                    else:\n",
    "                        yield batch\n",
    "                    # Reset\n",
    "                    idx = 0\n",
    "\n",
    "        # Provide remainder\n",
    "        if idx > 0:\n",
    "            if neighborhood:\n",
    "                max_length = max(neg_length[:idx].max(), pos_length[:idx].max())\n",
    "                yield batch[:idx], pos_neighbor[:idx, :max_length], pos_length[:idx], \\\n",
    "                      neg_neighbor[:idx, :max_length], neg_length[:idx]\n",
    "            else:\n",
    "                yield batch[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:20.769347Z",
     "start_time": "2019-07-09T21:39:20.133755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16980 5551 311\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(config.dataset)\n",
    "\n",
    "config.item_count = dataset.item_count\n",
    "config.user_count = dataset.user_count\n",
    "config.max_neighbors = dataset._max_user_neighbors\n",
    "\n",
    "print(dataset.item_count, dataset.user_count, dataset._max_user_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:22.165161Z",
     "start_time": "2019-07-09T21:39:22.154514Z"
    },
    "code_folding": [
     1,
     4,
     13
    ]
   },
   "outputs": [],
   "source": [
    "class LossLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossLayer, self).__init__()\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: predicted value\n",
    "        :param y: ground truth\n",
    "        :returns: Loss\n",
    "        \"\"\"        \n",
    "        bprl = torch.squeeze(self.bpr_loss(X, y))        \n",
    "        return bprl\n",
    "    \n",
    "    def bpr_loss(self, positive, negative):\n",
    "        r\"\"\"\n",
    "        Pairwise Loss from Bayesian Personalized Ranking.\n",
    "\n",
    "        \\log \\sigma(pos - neg)\n",
    "\n",
    "        where \\sigma is the sigmoid function, we try to set the ranking\n",
    "\n",
    "        if pos > neg = + number\n",
    "        if neg < pos = - number\n",
    "\n",
    "        Then applying the sigmoid to obtain a monotonically increasing function. Any\n",
    "        monotonically increasing function could be used, eg piecewise or probit.\n",
    "\n",
    "        :param positive: Score of prefered example\n",
    "        :param negative: Score of negative example\n",
    "        :param name: str, name scope\n",
    "        :returns: mean loss\n",
    "        \"\"\"\n",
    "        difference = positive - negative\n",
    "        # Numerical stability\n",
    "        eps = 1e-12\n",
    "        loss = -1*torch.log(torch.sigmoid(difference) + eps)\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:23.169573Z",
     "start_time": "2019-07-09T21:39:23.139559Z"
    },
    "code_folding": [
     1,
     15,
     52,
     111
    ]
   },
   "outputs": [],
   "source": [
    "class VariableLengthMemoryLayer(nn.Module):\n",
    "    def __init__(self, hops, embed_size):\n",
    "        super(VariableLengthMemoryLayer, self).__init__()\n",
    "        \n",
    "        self.hops = hops\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.hop_mapping = {}\n",
    "        for h in range(hops-1):\n",
    "            self.hop_mapping[h+1] = nn.Linear(self.embed_size, self.embed_size, bias=True).to(device)\n",
    "            self.hop_mapping[h+1].weight.requires_grad = True\n",
    "            self.hop_mapping[h+1].bias.requires_grad = True\n",
    "            nn.init.kaiming_normal_(self.hop_mapping[h+1].weight)\n",
    "            self.hop_mapping[h+1].bias.data.fill_(1.0)\n",
    "    \n",
    "    def mask_mod(self, inputs, mask_length, maxlen=None):\n",
    "        \"\"\"\n",
    "        Apply a memory mask such that the values we mask result in being the\n",
    "        minimum possible value we can represent with a float32.\n",
    "\n",
    "        :param inputs: [batch size, length], dtype=tf.float32\n",
    "        :param memory_mask: [batch_size] shape Tensor of ints indicating the\n",
    "            length of inputs\n",
    "        :param maxlen: Sets the maximum length of the sequence; if None infered\n",
    "            from inputs\n",
    "        :returns: [batch size, length] dim Tensor with the mask applied\n",
    "        \"\"\"\n",
    "        # [batch_size, length] => Sequence Mask\n",
    "        memory_mask = torch.arange(maxlen).to(device).expand(len(mask_length), maxlen) < mask_length.unsqueeze(1)\n",
    "        memory_mask = memory_mask.float()\n",
    "\n",
    "        num_remaining_memory_slots = torch.sum(memory_mask, 1)\n",
    "\n",
    "        # Get the numerical limits of a float\n",
    "        finfo = np.finfo(np.float32)\n",
    "        # print(finfo)\n",
    "\n",
    "        # If True = 1 = Keep that memory slot\n",
    "        kept_indices = memory_mask\n",
    "\n",
    "        # Inverse\n",
    "        ignored_indices = memory_mask < 1\n",
    "        ignored_indices = ignored_indices.float()\n",
    "\n",
    "        # If we keep the indices its the max float value else its the\n",
    "        # minimum float value. Then we can take the minimum\n",
    "        lower_bound = finfo.max * kept_indices + finfo.min * ignored_indices\n",
    "        slice_length = torch.max(mask_length)\n",
    "        \n",
    "        # Return the elementwise\n",
    "        return torch.min(inputs[:, :slice_length], lower_bound[:, :slice_length])\n",
    "        \n",
    "    def apply_attention_memory(self, memory, output_memory, query, memory_mask=None, maxlen=None):\n",
    "        \"\"\"\n",
    "            :param memory: [batch size, max length, embedding size],\n",
    "                typically Matrix M\n",
    "            :param output_memory: [batch size, max length, embedding size],\n",
    "                typically Matrix C\n",
    "            :param query: [batch size, embed size], typically u\n",
    "            :param memory_mask: [batch size] dim Tensor, the length of each\n",
    "                sequence if variable length\n",
    "            :param maxlen: int/Tensor, the maximum sequence padding length; if None it\n",
    "                infers based on the max of memory_mask\n",
    "            :returns: AttentionOutput\n",
    "                 output: [batch size, embedding size]\n",
    "                 weight: [batch size, max length], the attention weights applied to\n",
    "                         the output representation.\n",
    "        \"\"\"\n",
    "        # query = [batch size, embeddings] => expand => [batch size, embeddings, 1]\n",
    "        # transpose => [batch size, 1, embeddings]\n",
    "        query_expanded = query.unsqueeze(-1).transpose(2, 1)\n",
    "\n",
    "        # Apply batched dot product\n",
    "        # memory = [batch size, <Max Length>, Embeddings]\n",
    "        # Broadcast the same memory across each dimension of max length\n",
    "        # We obtain an attention value for each memory,\n",
    "        # ie a_0 p_0, a_1 p_1, .. a_n p_n, which equates to the max length\n",
    "        #    because our query is only 1 dim, we only get attention over memory\n",
    "        #    for that query. If our query was 2-d then we would obtain a matrix.\n",
    "        # Return: [batch size, max length]\n",
    "        batched_dot_prod = query_expanded * memory\n",
    "        scores = batched_dot_prod.sum(2)\n",
    "\n",
    "        if memory_mask is not None:\n",
    "            scores = self.mask_mod(scores, memory_mask, maxlen)\n",
    "\n",
    "        # Attention over memories: [Batch Size, <Max Length>]\n",
    "        # equation 2\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # [Batch Size, <Max Length>] => [Batch Size, 1, <Max Length>]\n",
    "        probs_temp = attention.unsqueeze(1)\n",
    "\n",
    "        # Output_Memories = [batch size, <Max Length>, Embeddings]\n",
    "        # Transpose = [Batch Size, Embedding Size, <Max Length>]\n",
    "        c_temp = output_memory.transpose(2, 1)\n",
    "\n",
    "        # Apply a weighted scalar or attention to the external memory \n",
    "        # to get weighted neighborhood\n",
    "        # [batch size, 1, <max length>] * [batch size, embedding size, <max length>]\n",
    "        neighborhood = c_temp * probs_temp\n",
    "\n",
    "        # Sum the weighted memories together\n",
    "        # Input:  [batch Size, embedding size, <max length>]\n",
    "        # Output: [Batch Size, Embedding Size]\n",
    "        # Weighted output vector\n",
    "        # equation 3\n",
    "        weighted_output = neighborhood.sum(2)\n",
    "\n",
    "        return {'weight':attention, 'output':weighted_output}\n",
    "    \n",
    "    def forward(self, query, memory, output_memory, seq_length, maxlen=32):\n",
    "        # find maximum length of sequences in this batch\n",
    "        cur_max = torch.max(seq_length).item()\n",
    "        # slice to max length\n",
    "        memory = memory[:, :cur_max]\n",
    "        output_memory = output_memory[:, :cur_max]\n",
    "        \n",
    "        user_query, item_query = query\n",
    "        hop_outputs = []\n",
    "        \n",
    "        # hop 0\n",
    "        # z = m_u + e_i\n",
    "        z = user_query + item_query\n",
    "        \n",
    "        for hop_k in range(self.hops):\n",
    "            # hop 1, ... , hop self.hops-1\n",
    "            if hop_k != 0:                \n",
    "                # f(Wz + o + b)\n",
    "                # equation 6\n",
    "                z = F.relu(self.hop_mapping[hop_k](z) + memory_hop['output'])\n",
    "            \n",
    "            # apply attention\n",
    "            memory_hop = self.apply_attention_memory(memory, \n",
    "                                               output_memory,\n",
    "                                               z, \n",
    "                                               seq_length, \n",
    "                                               maxlen)\n",
    "            hop_outputs.append(memory_hop)\n",
    "        \n",
    "        return hop_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:23.394192Z",
     "start_time": "2019-07-09T21:39:23.381711Z"
    },
    "code_folding": [
     2,
     17
    ]
   },
   "outputs": [],
   "source": [
    "class OutputModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size):\n",
    "        super(OutputModule, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.dense = nn.Linear(self.embed_size*2, self.embed_size, bias=True)\n",
    "        self.dense.weight.requires_grad = True\n",
    "        self.dense.bias.requires_grad = True\n",
    "        nn.init.kaiming_normal_(self.dense.weight)\n",
    "        self.dense.bias.data.fill_(1.0)\n",
    "        \n",
    "        self.out = nn.Linear(self.embed_size, 1, bias = False)\n",
    "        self.out.weight.requires_grad = True\n",
    "        nn.init.xavier_uniform_(self.out.weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = F.relu(self.dense(inputs))\n",
    "        output = self.out(output)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:23.686839Z",
     "start_time": "2019-07-09T21:39:23.665758Z"
    },
    "code_folding": [
     0,
     2,
     25
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CollaborativeMemoryNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, user_embeddings, item_embeddings):\n",
    "        super(CollaborativeMemoryNetwork, self).__init__()\n",
    "\n",
    "        # MemoryEmbed\n",
    "        self.user_memory = nn.Embedding(user_embeddings.shape[0], user_embeddings.shape[1])\n",
    "        self.user_memory.weight = nn.Parameter(torch.from_numpy(user_embeddings))\n",
    "        self.user_memory.weight.requires_grad = True\n",
    "        \n",
    "        # ItemMemory\n",
    "        self.item_memory = nn.Embedding(item_embeddings.shape[0], item_embeddings.shape[1])\n",
    "        self.item_memory.weight = nn.Parameter(torch.from_numpy(item_embeddings))\n",
    "        self.item_memory.weight.requires_grad = True\n",
    "\n",
    "        # MemoryOutput\n",
    "        self.user_output = nn.Embedding(user_embeddings.shape[0], user_embeddings.shape[1])\n",
    "        # user_output is initialised with tf.truncated_normal_initializer(stddev=0.01)}\n",
    "        self.user_output.weight.requires_grad = True\n",
    "\n",
    "        self.mem_layer = VariableLengthMemoryLayer(2, config.embed_size)\n",
    "\n",
    "        self.output_module = OutputModule(config.embed_size)\n",
    "\n",
    "    \n",
    "    def forward(self, input_users, input_items, input_items_negative, \n",
    "                input_neighborhoods, input_neighborhood_lengths, \n",
    "                input_neighborhoods_negative, input_neighborhood_lengths_negative, evaluation=False):\n",
    "        \n",
    "        # get embeddings from user memory\n",
    "        cur_user = self.user_memory(input_users)\n",
    "        cur_user_output = self.user_output(input_users)\n",
    "\n",
    "        # get embeddings from item memory\n",
    "        cur_item = self.item_memory(input_items)\n",
    "        \n",
    "        # queries\n",
    "        query = (cur_user, cur_item)\n",
    "        \n",
    "        # positive\n",
    "        neighbor = self.mem_layer(query, \n",
    "                                  self.user_memory(input_neighborhoods), \n",
    "                                  self.user_output(input_neighborhoods), \n",
    "                                  input_neighborhood_lengths, \n",
    "                                  config.max_neighbors)[-1]['output']\n",
    "        \n",
    "        score = self.output_module(torch.cat((cur_user * cur_item, neighbor), 1))\n",
    "        \n",
    "        \n",
    "        if evaluation:\n",
    "            return score\n",
    "        \n",
    "        cur_item_negative = self.item_memory(input_items_negative)\n",
    "        neg_query = (cur_user, cur_item_negative)\n",
    "            \n",
    "        # negative\n",
    "        neighbor_negative = self.mem_layer(neg_query, \n",
    "                                           self.user_memory(input_neighborhoods_negative), \n",
    "                                           self.user_output(input_neighborhoods_negative), \n",
    "                                           input_neighborhood_lengths_negative, \n",
    "                                           config.max_neighbors)[-1]['output']\n",
    "        \n",
    "        negative_output = self.output_module(torch.cat((cur_user * cur_item_negative, \n",
    "                                                        neighbor_negative), 1))\n",
    "        \n",
    "        return score, negative_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:27.739165Z",
     "start_time": "2019-07-09T21:39:24.634725Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# loading pretrained embeddings\n",
    "embeddings = np.load(config.pretrain, allow_pickle=True)\n",
    "\n",
    "# initialize model\n",
    "model = CollaborativeMemoryNetwork(embeddings['user']*0.5, embeddings['item']*0.5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T21:39:31.826641Z",
     "start_time": "2019-07-09T21:39:31.795626Z"
    },
    "code_folding": [
     0,
     41,
     56
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def get_model_scores(test_data, neighborhood, max_neighbors, return_scores=False):\n",
    "    \"\"\"\n",
    "    test_data = dict([positive, np.array[negatives]])\n",
    "    \"\"\"\n",
    "    out = ''\n",
    "    scores = []\n",
    "    progress = tqdm(test_data.items(), total=len(test_data),\n",
    "                    leave=False, desc=u'Evaluate || ')\n",
    "    for user, (pos, neg) in progress:\n",
    "        item_indices = list(neg) + [pos]\n",
    "\n",
    "        input_users = torch.LongTensor([user] * (len(neg) + 1)).to(device)\n",
    "        input_items = torch.LongTensor(item_indices).to(device)\n",
    "\n",
    "        if neighborhood is not None:\n",
    "            neighborhoods, neighborhood_length = (np.zeros((len(neg) + 1, max_neighbors), dtype=np.int32), \n",
    "                                                  np.ones(len(neg) + 1, dtype=np.int32))\n",
    "\n",
    "            for _idx, item in enumerate(item_indices):\n",
    "                _len = min(len(neighborhood.get(item, [])), max_neighbors)\n",
    "                if _len > 0:\n",
    "                    neighborhoods[_idx, :_len] = neighborhood[item][:_len]\n",
    "                    neighborhood_length[_idx] = _len\n",
    "                else:\n",
    "                    neighborhoods[_idx, :1] = user\n",
    "                    \n",
    "            input_neighborhoods = torch.LongTensor(neighborhoods).to(device)\n",
    "            input_neighborhood_lengths = torch.LongTensor(neighborhood_length).to(device)\n",
    "\n",
    "        score = model(input_users, input_items, None, input_neighborhoods, \n",
    "                      input_neighborhood_lengths, None, None, evaluation=True)\n",
    "        \n",
    "        scores.append(score.cpu().detach().numpy().ravel())\n",
    "        if return_scores:\n",
    "            s = ' '.join([\"{}:{}\".format(n, s) for s, n in zip(score.cpu().detach().numpy().ravel().tolist(), item_indices)])\n",
    "            out += \"{}\\t{}\\n\".format(user, s)\n",
    "    if return_scores:\n",
    "        return scores, out\n",
    "    return scores\n",
    "\n",
    "\n",
    "def evaluate_model(test_data, neighborhood, max_neighbors, EVAL_AT=[1, 5, 10]):\n",
    "    scores = get_model_scores(test_data, neighborhood, max_neighbors)\n",
    "    hrs = []\n",
    "    ndcgs = []\n",
    "    s = '\\n'\n",
    "    for k in EVAL_AT:\n",
    "        hr, ndcg = get_eval(scores, len(scores[0]) - 1, k)\n",
    "        s += \"{:<14} {:<14.6f}{:<14} {:.6f}\\n\".format('HR@%s' % k, hr, 'NDCG@%s' % k, ndcg)\n",
    "        hrs.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    print(s + '\\n')\n",
    "\n",
    "    return hrs, ndcgs\n",
    "\n",
    "\n",
    "def get_eval(scores, index, top_n=10):\n",
    "    \"\"\"\n",
    "    if the last element is the correct one, then\n",
    "    index = len(scores[0])-1\n",
    "    \"\"\"\n",
    "    ndcg = 0.0\n",
    "    hr = 0.0\n",
    "    assert len(scores[0]) > index and index >= 0\n",
    "\n",
    "    for score in scores:\n",
    "        # Get the top n indices\n",
    "        arg_index = np.argsort(-score)[:top_n]\n",
    "        if index in arg_index:\n",
    "            # Get the position\n",
    "            ndcg += np.log(2.0) / np.log(arg_index.tolist().index(index) + 2.0)\n",
    "            # Increment\n",
    "            hr += 1.0\n",
    "\n",
    "    return hr / len(scores), ndcg / len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-07-09T21:42:18.780Z"
    },
    "code_folding": [
     9
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture training_loop_output\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate, \n",
    "                                momentum=config.momentum)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.decay_rate)\n",
    "\n",
    "criterion = LossLayer()\n",
    "\n",
    "loss = []\n",
    "for i in range(config.epochs):    \n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Decay Learning Rate\n",
    "    scheduler.step()\n",
    "    # Print Learning Rate\n",
    "    print('Epoch:', i,'LR:', scheduler.get_lr())\n",
    "    \n",
    "    progress = tqdm(enumerate(dataset.get_data(config.batch_size, True, config.neg_count)), \n",
    "                    dynamic_ncols=True, total=(dataset.train_size * config.neg_count) // config.batch_size)\n",
    "    \n",
    "    for k, batch in progress:\n",
    "        \n",
    "        ratings, pos_neighborhoods, pos_neighborhood_length, neg_neighborhoods, neg_neighborhood_length = batch\n",
    "        \n",
    "        input_users = torch.LongTensor(np.array(ratings[:, 0], dtype=np.int32)).to(device)\n",
    "        input_items = torch.LongTensor(np.array(ratings[:, 1], dtype=np.int32)).to(device)\n",
    "        input_items_negative = torch.LongTensor(np.array(ratings[:, 2], dtype=np.int32)).to(device)\n",
    "        input_neighborhoods = torch.LongTensor(np.array(pos_neighborhoods, dtype=np.int32)).to(device)\n",
    "        input_neighborhood_lengths = torch.LongTensor(np.array(pos_neighborhood_length, dtype=np.int32)).to(device)\n",
    "        input_neighborhoods_negative = torch.LongTensor(np.array(neg_neighborhoods, dtype=np.int32)).to(device)\n",
    "        input_neighborhood_lengths_negative = torch.LongTensor(np.array(neg_neighborhood_length, dtype=np.int32)).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        score_pos, score_neg = model(input_users, input_items, input_items_negative, \n",
    "                                     input_neighborhoods, input_neighborhood_lengths, \n",
    "                                     input_neighborhoods_negative, input_neighborhood_lengths_negative)\n",
    "        \n",
    "        batch_loss = criterion(score_pos, score_neg)\n",
    "        \n",
    "        # adding l2 regularisation\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in ['mem_layer.hop_mapping.weight', \n",
    "                        'output_module.dense.weight', \n",
    "                        'output_module.out.weight']:\n",
    "                l2 = torch.sqrt(param.pow(2).sum())\n",
    "                batch_loss += (config.l2_lambda * l2)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # plot_grad_flow(model.named_parameters())\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss.append(batch_loss.item())\n",
    "        progress.set_description(u\"[{}] Loss: {:,.4f} » » » » \".format(i, batch_loss.item()))\n",
    "    \n",
    "    print(\"Epoch {}: Avg Loss/Batch {:<20,.6f}\".format(i, np.mean(loss)))\n",
    "    model.eval()\n",
    "    evaluate_model(dataset.test_data, dataset.item_users_list, config.max_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T23:37:32.379560Z",
     "start_time": "2019-07-09T23:37:05.861503Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture final_eval_output\n",
    "\n",
    "EVAL_AT = range(1, 11)\n",
    "hrs, ndcgs = [], []\n",
    "s = \"\"\n",
    "scores, out = get_model_scores(dataset.test_data, dataset.item_users_list, config.max_neighbors, True)\n",
    "\n",
    "for k in EVAL_AT:\n",
    "    hr, ndcg = get_eval(scores, len(scores[0])-1, k)\n",
    "    hrs.append(hr)\n",
    "    ndcgs.append(ndcg)\n",
    "    s += \"{:<14} {:<14.6f}{:<14} {:.6f}\\n\".format('HR@%s' % k, hr,\n",
    "                                                  'NDCG@%s' % k, ndcg)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T23:37:40.093370Z",
     "start_time": "2019-07-09T23:37:40.085467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training log...\n"
     ]
    }
   ],
   "source": [
    "print('Saving training log...')\n",
    "with open(\"{}{}\".format(config.logdir, config.version+'.log'), 'w') as fout:\n",
    "    header = ','.join([str(k) for k in EVAL_AT])\n",
    "    fout.write(\"{},{}\\n\".format('metric', header))\n",
    "    ndcg = ','.join([str(x) for x in ndcgs])\n",
    "    hr = ','.join([str(x) for x in hrs])\n",
    "    fout.write(\"ndcg,{}\\n\".format(ndcg))\n",
    "    fout.write(\"hr,{}\".format(hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T23:37:47.249378Z",
     "start_time": "2019-07-09T23:37:47.232034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "# save model weights\n",
    "print(\"Saving model...\")\n",
    "torch.save(model.state_dict(), config.ssdir+config.version+'.ss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T23:37:54.410130Z",
     "start_time": "2019-07-09T23:37:54.405735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 LR: [0.0008100000000000001]\n",
      "\n",
      "Epoch 0: Avg Loss/Batch 0.381786            \n",
      "\r\n",
      "HR@1           0.337957      NDCG@1         0.337957\n",
      "HR@5           0.696811      NDCG@5         0.529033\n",
      "HR@10          0.811385      NDCG@10        0.566494\n",
      "\n",
      "\n",
      "Epoch: 1 LR: [0.000729]\n",
      "\n",
      "Epoch 1: Avg Loss/Batch 0.315544            \n",
      "\r\n",
      "HR@1           0.360656      NDCG@1         0.360656\n",
      "HR@5           0.724014      NDCG@5         0.553438\n",
      "HR@10          0.842371      NDCG@10        0.592192\n",
      "\n",
      "\n",
      "Epoch: 2 LR: [0.0006561000000000001]\n",
      "\n",
      "Epoch 2: Avg Loss/Batch 0.277038            \n",
      "\r\n",
      "HR@1           0.379751      NDCG@1         0.379751\n",
      "HR@5           0.735723      NDCG@5         0.569441\n",
      "HR@10          0.850838      NDCG@10        0.606991\n",
      "\n",
      "\n",
      "Epoch: 3 LR: [0.00059049]\n",
      "\n",
      "Epoch 3: Avg Loss/Batch 0.250987            \n",
      "\r\n",
      "HR@1           0.382454      NDCG@1         0.382454\n",
      "HR@5           0.736624      NDCG@5         0.571432\n",
      "HR@10          0.860025      NDCG@10        0.611726\n",
      "\n",
      "\n",
      "Epoch: 4 LR: [0.000531441]\n",
      "\n",
      "Epoch 4: Avg Loss/Batch 0.231770            \n",
      "\r\n",
      "HR@1           0.383895      NDCG@1         0.383895\n",
      "HR@5           0.747433      NDCG@5         0.577005\n",
      "HR@10          0.863808      NDCG@10        0.614986\n",
      "\n",
      "\n",
      "Epoch: 5 LR: [0.0004782969]\n",
      "\n",
      "Epoch 5: Avg Loss/Batch 0.216839            \n",
      "\r\n",
      "HR@1           0.377590      NDCG@1         0.377590\n",
      "HR@5           0.745451      NDCG@5         0.574913\n",
      "HR@10          0.861286      NDCG@10        0.612653\n",
      "\n",
      "\n",
      "Epoch: 6 LR: [0.00043046721]\n",
      "\n",
      "Epoch 6: Avg Loss/Batch 0.204766            \n",
      "\r\n",
      "HR@1           0.375968      NDCG@1         0.375968\n",
      "HR@5           0.748514      NDCG@5         0.574678\n",
      "HR@10          0.861106      NDCG@10        0.611291\n",
      "\n",
      "\n",
      "Epoch: 7 LR: [0.000387420489]\n",
      "\n",
      "Epoch 7: Avg Loss/Batch 0.194806            \n",
      "\r\n",
      "HR@1           0.392902      NDCG@1         0.392902\n",
      "HR@5           0.761304      NDCG@5         0.589530\n",
      "HR@10          0.872095      NDCG@10        0.625576\n",
      "\n",
      "\n",
      "Epoch: 8 LR: [0.0003486784401]\n",
      "\n",
      "Epoch 8: Avg Loss/Batch 0.186362            \n",
      "\r\n",
      "HR@1           0.390560      NDCG@1         0.390560\n",
      "HR@5           0.756080      NDCG@5         0.586901\n",
      "HR@10          0.873897      NDCG@10        0.625272\n",
      "\n",
      "\n",
      "Epoch: 9 LR: [0.00031381059609000004]\n",
      "\n",
      "Epoch 9: Avg Loss/Batch 0.179108            \n",
      "\r\n",
      "HR@1           0.376148      NDCG@1         0.376148\n",
      "HR@5           0.744190      NDCG@5         0.574464\n",
      "HR@10          0.865250      NDCG@10        0.613767\n",
      "\n",
      "\n",
      "Epoch: 10 LR: [0.00028242953648100003]\n",
      "\n",
      "Epoch 10: Avg Loss/Batch 0.172799            \n",
      "\r\n",
      "HR@1           0.397406      NDCG@1         0.397406\n",
      "HR@5           0.762926      NDCG@5         0.592747\n",
      "HR@10          0.873897      NDCG@10        0.629045\n",
      "\n",
      "\n",
      "Epoch: 11 LR: [0.00025418658283290005]\n",
      "\n",
      "Epoch 11: Avg Loss/Batch 0.167219            \n",
      "\r\n",
      "HR@1           0.392902      NDCG@1         0.392902\n",
      "HR@5           0.761304      NDCG@5         0.591038\n",
      "HR@10          0.875158      NDCG@10        0.628180\n",
      "\n",
      "\n",
      "Epoch: 12 LR: [0.00022876792454961005]\n",
      "\n",
      "Epoch 12: Avg Loss/Batch 0.162261            \n",
      "\r\n",
      "HR@1           0.396145      NDCG@1         0.396145\n",
      "HR@5           0.758422      NDCG@5         0.590912\n",
      "HR@10          0.875518      NDCG@10        0.629223\n",
      "\n",
      "\n",
      "Epoch: 13 LR: [0.00020589113209464906]\n",
      "\n",
      "Epoch 13: Avg Loss/Batch 0.157870            \n",
      "\r\n",
      "HR@1           0.395965      NDCG@1         0.395965\n",
      "HR@5           0.764547      NDCG@5         0.593604\n",
      "HR@10          0.875698      NDCG@10        0.629905\n",
      "\n",
      "\n",
      "Epoch: 14 LR: [0.00018530201888518417]\n",
      "\n",
      "Epoch 14: Avg Loss/Batch 0.153874            \n",
      "\r\n",
      "HR@1           0.396685      NDCG@1         0.396685\n",
      "HR@5           0.762926      NDCG@5         0.592844\n",
      "HR@10          0.876419      NDCG@10        0.629891\n",
      "\n",
      "\n",
      "Epoch: 15 LR: [0.00016677181699666576]\n",
      "\n",
      "Epoch 15: Avg Loss/Batch 0.150269            \n",
      "\r\n",
      "HR@1           0.393623      NDCG@1         0.393623\n",
      "HR@5           0.762385      NDCG@5         0.590795\n",
      "HR@10          0.879481      NDCG@10        0.628880\n",
      "\n",
      "\n",
      "Epoch: 16 LR: [0.0001500946352969992]\n",
      "\n",
      "Epoch 16: Avg Loss/Batch 0.147012            \n",
      "\r\n",
      "HR@1           0.398487      NDCG@1         0.398487\n",
      "HR@5           0.769231      NDCG@5         0.597224\n",
      "HR@10          0.880562      NDCG@10        0.633543\n",
      "\n",
      "\n",
      "Epoch: 17 LR: [0.0001350851717672993]\n",
      "\n",
      "Epoch 17: Avg Loss/Batch 0.144025            \n",
      "\r\n",
      "HR@1           0.399928      NDCG@1         0.399928\n",
      "HR@5           0.770492      NDCG@5         0.598793\n",
      "HR@10          0.882003      NDCG@10        0.635185\n",
      "\n",
      "\n",
      "Epoch: 18 LR: [0.00012157665459056936]\n",
      "\n",
      "Epoch 18: Avg Loss/Batch 0.141281            \n",
      "\r\n",
      "HR@1           0.395424      NDCG@1         0.395424\n",
      "HR@5           0.762025      NDCG@5         0.592186\n",
      "HR@10          0.878400      NDCG@10        0.630166\n",
      "\n",
      "\n",
      "Epoch: 19 LR: [0.00010941898913151243]\n",
      "\n",
      "Epoch 19: Avg Loss/Batch 0.138744            \n",
      "\r\n",
      "HR@1           0.386237      NDCG@1         0.386237\n",
      "HR@5           0.752297      NDCG@5         0.582460\n",
      "HR@10          0.872455      NDCG@10        0.621594\n",
      "\n",
      "\n",
      "Epoch: 20 LR: [9.847709021836118e-05]\n",
      "\n",
      "Epoch 20: Avg Loss/Batch 0.136398            \n",
      "\r\n",
      "HR@1           0.400468      NDCG@1         0.400468\n",
      "HR@5           0.770132      NDCG@5         0.598333\n",
      "HR@10          0.881823      NDCG@10        0.634619\n",
      "\n",
      "\n",
      "Epoch: 21 LR: [8.862938119652506e-05]\n",
      "\n",
      "Epoch 21: Avg Loss/Batch 0.134211            \n",
      "\r\n",
      "HR@1           0.402810      NDCG@1         0.402810\n",
      "HR@5           0.772654      NDCG@5         0.601446\n",
      "HR@10          0.882724      NDCG@10        0.637342\n",
      "\n",
      "\n",
      "Epoch: 22 LR: [7.976644307687256e-05]\n",
      "\n",
      "Epoch 22: Avg Loss/Batch 0.132190            \n",
      "\r\n",
      "HR@1           0.399387      NDCG@1         0.399387\n",
      "HR@5           0.769591      NDCG@5         0.598167\n",
      "HR@10          0.884345      NDCG@10        0.635394\n",
      "\n",
      "\n",
      "Epoch: 23 LR: [7.17897987691853e-05]\n",
      "\n",
      "Epoch 23: Avg Loss/Batch 0.130301            \n",
      "\r\n",
      "HR@1           0.398307      NDCG@1         0.398307\n",
      "HR@5           0.771032      NDCG@5         0.598801\n",
      "HR@10          0.882364      NDCG@10        0.635081\n",
      "\n",
      "\n",
      "Epoch: 24 LR: [6.461081889226677e-05]\n",
      "\n",
      "Epoch 24: Avg Loss/Batch 0.128546            \n",
      "\r\n",
      "HR@1           0.397946      NDCG@1         0.397946\n",
      "HR@5           0.767970      NDCG@5         0.597409\n",
      "HR@10          0.880922      NDCG@10        0.634252\n",
      "\n",
      "\n",
      "Epoch: 25 LR: [5.81497370030401e-05]\n",
      "\n",
      "Epoch 25: Avg Loss/Batch 0.126888            \n",
      "\r\n",
      "HR@1           0.402810      NDCG@1         0.402810\n",
      "HR@5           0.770672      NDCG@5         0.600198\n",
      "HR@10          0.882544      NDCG@10        0.636662\n",
      "\n",
      "\n",
      "Epoch: 26 LR: [5.233476330273609e-05]\n",
      "\n",
      "Epoch 26: Avg Loss/Batch 0.125342            \n",
      "\r\n",
      "HR@1           0.399387      NDCG@1         0.399387\n",
      "HR@5           0.764727      NDCG@5         0.595871\n",
      "HR@10          0.882544      NDCG@10        0.634261\n",
      "\n",
      "\n",
      "Epoch: 27 LR: [4.7101286972462485e-05]\n",
      "\n",
      "Epoch 27: Avg Loss/Batch 0.123893            \n",
      "\r\n",
      "HR@1           0.395424      NDCG@1         0.395424\n",
      "HR@5           0.763826      NDCG@5         0.593685\n",
      "HR@10          0.881283      NDCG@10        0.631951\n",
      "\n",
      "\n",
      "Epoch: 28 LR: [4.239115827521624e-05]\n",
      "\n",
      "Epoch 28: Avg Loss/Batch 0.122530            \n",
      "\r\n",
      "HR@1           0.401369      NDCG@1         0.401369\n",
      "HR@5           0.773014      NDCG@5         0.601029\n",
      "HR@10          0.882904      NDCG@10        0.636854\n",
      "\n",
      "\n",
      "Epoch: 29 LR: [3.8152042447694614e-05]\n",
      "\n",
      "Epoch 29: Avg Loss/Batch 0.121250            \n",
      "\r\n",
      "HR@1           0.393262      NDCG@1         0.393262\n",
      "HR@5           0.762745      NDCG@5         0.591482\n",
      "HR@10          0.881643      NDCG@10        0.630098\n",
      "\n",
      "\n",
      "Final Evaluation:\n",
      "\n",
      "\r",
      "HR@1           0.393262      NDCG@1         0.393262\n",
      "HR@2           0.563502      NDCG@2         0.500672\n",
      "HR@3           0.662403      NDCG@3         0.550122\n",
      "HR@4           0.720411      NDCG@4         0.575105\n",
      "HR@5           0.762745      NDCG@5         0.591482\n",
      "HR@6           0.796974      NDCG@6         0.603674\n",
      "HR@7           0.822735      NDCG@7         0.612261\n",
      "HR@8           0.844713      NDCG@8         0.619195\n",
      "HR@9           0.863808      NDCG@9         0.624943\n",
      "HR@10          0.881643      NDCG@10        0.630098\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"{}Final Evaluation:\\n\\n{}\".format(str(training_loop_output), str(final_eval_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T23:38:01.579335Z",
     "start_time": "2019-07-09T23:38:01.575196Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging complete training progress...\n"
     ]
    }
   ],
   "source": [
    "print('Logging complete training progress...')\n",
    "with open(\"{}{}\".format(config.logdir, config.version+'_verbose.log'), 'w') as fout:\n",
    "    fout.write(\"{}Final Evaluation:\\n\\n{}\".format(str(training_loop_output), str(final_eval_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T15:14:00.840314Z",
     "start_time": "2019-07-09T15:13:40.189Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model weights\n",
    "print('Loading model...')\n",
    "model = CollaborativeMemoryNetwork(embeddings['user']*0.5, embeddings['item']*0.5)\n",
    "model.load_state_dict(torch.load(config.logdir+config.version))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
