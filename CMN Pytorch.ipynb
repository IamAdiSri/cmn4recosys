{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Collaborative Memory Network for Recommendation Systems\n",
    "_**Ebesu, Shen, Fang** - The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval - SIGIR '18_\n",
    "\n",
    "\n",
    "This notebook by **Aditya Srivastava** is a PyTorch port to the TensorFlow code originally by the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:28:55.608949Z",
     "start_time": "2019-07-09T13:28:55.106371Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:28:56.039079Z",
     "start_time": "2019-07-09T13:28:55.699791Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:28:56.495121Z",
     "start_time": "2019-07-09T13:28:56.488595Z"
    }
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    resume = False\n",
    "    ssdir = 'snapshots/'\n",
    "    logdir = 'logs/'\n",
    "    version = 'model_0'\n",
    "    dataset = 'data/citeulike-a.npz'\n",
    "    pretrain = 'pretrain/citeulike-a_e50.npz'\n",
    "    embed_size = 50\n",
    "    epochs = 5 # training epochs (originally 30)\n",
    "    batch_size = 128\n",
    "    hops = 2 # number of hops/layers\n",
    "    l2_lambda = 0.1 # l2 regularization\n",
    "    neg_count = 4 # negative samples count\n",
    "    optimizer = 'rmsprop'\n",
    "    learning_rate = 0.001\n",
    "    decay_rate = 0.9\n",
    "    momentum = 0.9\n",
    "    grad_clip = 5.0\n",
    "    tol = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:28:58.157367Z",
     "start_time": "2019-07-09T13:28:58.139102Z"
    },
    "code_folding": [
     0
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:28:59.291877Z",
     "start_time": "2019-07-09T13:28:59.251313Z"
    },
    "code_folding": [
     2,
     27,
     35,
     42,
     48,
     54,
     72,
     83
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"\n",
    "        Wraps dataset and produces batches for the model to consume\n",
    "\n",
    "        :param filename: path to training data for npz file\n",
    "        \"\"\"\n",
    "        self._data = np.load(filename, allow_pickle=True)\n",
    "        self.train_data = self._data['train_data'][:, :2]\n",
    "        self.test_data = self._data['test_data'].tolist()\n",
    "        self._train_index = np.arange(len(self.train_data), dtype=np.uint)\n",
    "        self._n_users, self._n_items = self.train_data.max(axis=0) + 1\n",
    "\n",
    "        # Neighborhoods\n",
    "        self.user_items = defaultdict(set)\n",
    "        self.item_users = defaultdict(set)\n",
    "        for u, i in self.train_data:\n",
    "            self.user_items[u].add(i)\n",
    "            self.item_users[i].add(u)\n",
    "        # Get a list version so we do not need to perform type casting\n",
    "        self.item_users_list = {k: list(v) for k, v in self.item_users.items()}\n",
    "        self._max_user_neighbors = max([len(x) for x in self.item_users.values()])\n",
    "        self.user_items = dict(self.user_items)\n",
    "        self.item_users = dict(self.item_users)\n",
    "\n",
    "    @property\n",
    "    def train_size(self):\n",
    "        \"\"\"\n",
    "        :return: number of examples in training set\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return len(self.train_data)\n",
    "\n",
    "    @property\n",
    "    def user_count(self):\n",
    "        \"\"\"\n",
    "        Number of users in dataset\n",
    "        \"\"\"\n",
    "        return self._n_users\n",
    "\n",
    "    @property\n",
    "    def item_count(self):\n",
    "        \"\"\"\n",
    "        Number of items in dataset\n",
    "        \"\"\"\n",
    "        return self._n_items\n",
    "\n",
    "    def _sample_item(self):\n",
    "        \"\"\"\n",
    "        Draw an item uniformly\n",
    "        \"\"\"\n",
    "        return np.random.randint(0, self.item_count)\n",
    "\n",
    "    def _sample_negative_item(self, user_id):\n",
    "        \"\"\"\n",
    "        Uniformly sample a negative item\n",
    "        \"\"\"\n",
    "        if user_id > self.user_count:\n",
    "            raise ValueError(\"Trying to sample user id: {} > user count: {}\".format(\n",
    "                user_id, self.user_count))\n",
    "\n",
    "        n = self._sample_item()\n",
    "        positive_items = self.user_items[user_id]\n",
    "\n",
    "        if len(positive_items) >= self.item_count:\n",
    "            raise ValueError(\"The User has rated more items than possible %s / %s\" % (\n",
    "                len(positive_items), self.item_count))\n",
    "        while n in positive_items or n not in self.item_users:\n",
    "            n = self._sample_item()\n",
    "        return n\n",
    "\n",
    "    def _generate_data(self, neg_count):\n",
    "        idx = 0\n",
    "        self._examples = np.zeros((self.train_size*neg_count, 3),\n",
    "                                  dtype=np.uint32)\n",
    "        self._examples[:, :] = 0\n",
    "        for user_idx, item_idx in self.train_data:\n",
    "            for _ in range(neg_count):\n",
    "                neg_item_idx = self._sample_negative_item(user_idx)\n",
    "                self._examples[idx, :] = [user_idx, item_idx, neg_item_idx]\n",
    "                idx += 1\n",
    "\n",
    "    def get_data(self, batch_size: int, neighborhood: bool, neg_count: int):\n",
    "        \"\"\"\n",
    "        Batch data together as (user, item, negative item), pos_neighborhood,\n",
    "        length of neighborhood, negative_neighborhood, length of negative neighborhood\n",
    "\n",
    "        if neighborhood is False returns only user, item, negative_item so we\n",
    "        can reuse this for non-neighborhood-based methods.\n",
    "\n",
    "        :param batch_size: size of the batch\n",
    "        :param neighborhood: return the neighborhood information or not\n",
    "        :param neg_count: number of negative samples to uniformly draw per a pos\n",
    "                          example\n",
    "        :return: generator\n",
    "        \"\"\"\n",
    "        # Allocate inputs\n",
    "        batch = np.zeros((batch_size, 3), dtype=np.uint32)\n",
    "        pos_neighbor = np.zeros((batch_size, self._max_user_neighbors), dtype=np.int32)\n",
    "        pos_length = np.zeros(batch_size, dtype=np.int32)\n",
    "        neg_neighbor = np.zeros((batch_size, self._max_user_neighbors), dtype=np.int32)\n",
    "        neg_length = np.zeros(batch_size, dtype=np.int32)\n",
    "\n",
    "        # Shuffle index\n",
    "        np.random.shuffle(self._train_index)\n",
    "\n",
    "        idx = 0\n",
    "        for user_idx, item_idx in self.train_data[self._train_index]:\n",
    "            # TODO: set positive values outside of for loop\n",
    "            for _ in range(neg_count):\n",
    "                neg_item_idx = self._sample_negative_item(user_idx)\n",
    "                batch[idx, :] = [user_idx, item_idx, neg_item_idx]\n",
    "\n",
    "                # Get neighborhood information\n",
    "                if neighborhood:\n",
    "                    if len(self.item_users.get(item_idx, [])) > 0:\n",
    "                        pos_length[idx] = len(self.item_users[item_idx])\n",
    "                        pos_neighbor[idx, :pos_length[idx]] = self.item_users_list[item_idx]\n",
    "                    else:\n",
    "                        # Length defaults to 1\n",
    "                        pos_length[idx] = 1\n",
    "                        pos_neighbor[idx, 0] = item_idx\n",
    "\n",
    "                    if len(self.item_users.get(neg_item_idx, [])) > 0:\n",
    "                        neg_length[idx] = len(self.item_users[neg_item_idx])\n",
    "                        neg_neighbor[idx, :neg_length[idx]] = self.item_users_list[neg_item_idx]\n",
    "                    else:\n",
    "                        # Length defaults to 1\n",
    "                        neg_length[idx] = 1\n",
    "                        neg_neighbor[idx, 0] = neg_item_idx\n",
    "\n",
    "                idx += 1\n",
    "                # Yield batch if we filled queue\n",
    "                if idx == batch_size:\n",
    "                    if neighborhood:\n",
    "                        max_length = max(neg_length.max(), pos_length.max())\n",
    "                        yield batch, pos_neighbor[:, :max_length], pos_length, \\\n",
    "                              neg_neighbor[:, :max_length], neg_length\n",
    "                        pos_length[:] = 1\n",
    "                        neg_length[:] = 1\n",
    "                    else:\n",
    "                        yield batch\n",
    "                    # Reset\n",
    "                    idx = 0\n",
    "\n",
    "        # Provide remainder\n",
    "        if idx > 0:\n",
    "            if neighborhood:\n",
    "                max_length = max(neg_length[:idx].max(), pos_length[:idx].max())\n",
    "                yield batch[:idx], pos_neighbor[:idx, :max_length], pos_length[:idx], \\\n",
    "                      neg_neighbor[:idx, :max_length], neg_length[:idx]\n",
    "            else:\n",
    "                yield batch[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:29:00.628533Z",
     "start_time": "2019-07-09T13:29:00.032445Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(config.dataset)\n",
    "\n",
    "config.item_count = dataset.item_count\n",
    "config.user_count = dataset.user_count\n",
    "config.max_neighbors = dataset._max_user_neighbors\n",
    "\n",
    "print(dataset.item_count, dataset.user_count, dataset._max_user_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:29:03.498887Z",
     "start_time": "2019-07-09T13:29:03.488033Z"
    },
    "code_folding": [
     1,
     4,
     13
    ]
   },
   "outputs": [],
   "source": [
    "class LossLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossLayer, self).__init__()\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: predicted value\n",
    "        :param y: ground truth\n",
    "        :returns: Loss\n",
    "        \"\"\"        \n",
    "        bprl = torch.squeeze(self.bpr_loss(X, y))        \n",
    "        return bprl\n",
    "    \n",
    "    def bpr_loss(self, positive, negative):\n",
    "        r\"\"\"\n",
    "        Pairwise Loss from Bayesian Personalized Ranking.\n",
    "\n",
    "        \\log \\sigma(pos - neg)\n",
    "\n",
    "        where \\sigma is the sigmoid function, we try to set the ranking\n",
    "\n",
    "        if pos > neg = + number\n",
    "        if neg < pos = - number\n",
    "\n",
    "        Then applying the sigmoid to obtain a monotonically increasing function. Any\n",
    "        monotonically increasing function could be used, eg piecewise or probit.\n",
    "\n",
    "        :param positive: Score of prefered example\n",
    "        :param negative: Score of negative example\n",
    "        :param name: str, name scope\n",
    "        :returns: mean loss\n",
    "        \"\"\"\n",
    "        difference = positive - negative\n",
    "        # Numerical stability\n",
    "        eps = 1e-12\n",
    "        loss = -1*torch.log(torch.sigmoid(difference) + eps)\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:29:05.186315Z",
     "start_time": "2019-07-09T13:29:05.154514Z"
    },
    "code_folding": [
     1,
     15,
     52,
     111
    ]
   },
   "outputs": [],
   "source": [
    "class VariableLengthMemoryLayer(nn.Module):\n",
    "    def __init__(self, hops, embed_size):\n",
    "        super(VariableLengthMemoryLayer, self).__init__()\n",
    "        \n",
    "        self.hops = hops\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.hop_mapping = {}\n",
    "        for h in range(hops-1):\n",
    "            self.hop_mapping[h+1] = nn.Linear(self.embed_size, self.embed_size, bias=True)\n",
    "            self.hop_mapping[h+1].weight.requires_grad = True\n",
    "            self.hop_mapping[h+1].bias.requires_grad = True\n",
    "            nn.init.kaiming_normal_(self.hop_mapping[h+1].weight)\n",
    "            self.hop_mapping[h+1].bias.data.fill_(1.0)\n",
    "    \n",
    "    def mask_mod(self, inputs, mask_length, maxlen=None):\n",
    "        \"\"\"\n",
    "        Apply a memory mask such that the values we mask result in being the\n",
    "        minimum possible value we can represent with a float32.\n",
    "\n",
    "        :param inputs: [batch size, length], dtype=tf.float32\n",
    "        :param memory_mask: [batch_size] shape Tensor of ints indicating the\n",
    "            length of inputs\n",
    "        :param maxlen: Sets the maximum length of the sequence; if None infered\n",
    "            from inputs\n",
    "        :returns: [batch size, length] dim Tensor with the mask applied\n",
    "        \"\"\"\n",
    "        # [batch_size, length] => Sequence Mask\n",
    "        memory_mask = torch.arange(maxlen).expand(len(mask_length), maxlen) < mask_length.unsqueeze(1)\n",
    "        memory_mask = memory_mask.float()\n",
    "\n",
    "        num_remaining_memory_slots = torch.sum(memory_mask, 1)\n",
    "\n",
    "        # Get the numerical limits of a float\n",
    "        finfo = np.finfo(np.float32)\n",
    "        # print(finfo)\n",
    "\n",
    "        # If True = 1 = Keep that memory slot\n",
    "        kept_indices = memory_mask\n",
    "\n",
    "        # Inverse\n",
    "        ignored_indices = memory_mask < 1\n",
    "        ignored_indices = ignored_indices.float()\n",
    "\n",
    "        # If we keep the indices its the max float value else its the\n",
    "        # minimum float value. Then we can take the minimum\n",
    "        lower_bound = finfo.max * kept_indices + finfo.min * ignored_indices\n",
    "        slice_length = torch.max(mask_length)\n",
    "        \n",
    "        # Return the elementwise\n",
    "        return torch.min(inputs[:, :slice_length], lower_bound[:, :slice_length])\n",
    "        \n",
    "    def apply_attention_memory(self, memory, output_memory, query, memory_mask=None, maxlen=None):\n",
    "        \"\"\"\n",
    "            :param memory: [batch size, max length, embedding size],\n",
    "                typically Matrix M\n",
    "            :param output_memory: [batch size, max length, embedding size],\n",
    "                typically Matrix C\n",
    "            :param query: [batch size, embed size], typically u\n",
    "            :param memory_mask: [batch size] dim Tensor, the length of each\n",
    "                sequence if variable length\n",
    "            :param maxlen: int/Tensor, the maximum sequence padding length; if None it\n",
    "                infers based on the max of memory_mask\n",
    "            :returns: AttentionOutput\n",
    "                 output: [batch size, embedding size]\n",
    "                 weight: [batch size, max length], the attention weights applied to\n",
    "                         the output representation.\n",
    "        \"\"\"\n",
    "        # query = [batch size, embeddings] => expand => [batch size, embeddings, 1]\n",
    "        # transpose => [batch size, 1, embeddings]\n",
    "        query_expanded = query.unsqueeze(-1).transpose(2, 1)\n",
    "\n",
    "        # Apply batched dot product\n",
    "        # memory = [batch size, <Max Length>, Embeddings]\n",
    "        # Broadcast the same memory across each dimension of max length\n",
    "        # We obtain an attention value for each memory,\n",
    "        # ie a_0 p_0, a_1 p_1, .. a_n p_n, which equates to the max length\n",
    "        #    because our query is only 1 dim, we only get attention over memory\n",
    "        #    for that query. If our query was 2-d then we would obtain a matrix.\n",
    "        # Return: [batch size, max length]\n",
    "        batched_dot_prod = query_expanded * memory\n",
    "        scores = batched_dot_prod.sum(2)\n",
    "\n",
    "        if memory_mask is not None:\n",
    "            scores = self.mask_mod(scores, memory_mask, maxlen)\n",
    "\n",
    "        # Attention over memories: [Batch Size, <Max Length>]\n",
    "        # equation 2\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # [Batch Size, <Max Length>] => [Batch Size, 1, <Max Length>]\n",
    "        probs_temp = attention.unsqueeze(1)\n",
    "\n",
    "        # Output_Memories = [batch size, <Max Length>, Embeddings]\n",
    "        # Transpose = [Batch Size, Embedding Size, <Max Length>]\n",
    "        c_temp = output_memory.transpose(2, 1)\n",
    "\n",
    "        # Apply a weighted scalar or attention to the external memory \n",
    "        # to get weighted neighborhood\n",
    "        # [batch size, 1, <max length>] * [batch size, embedding size, <max length>]\n",
    "        neighborhood = c_temp * probs_temp\n",
    "\n",
    "        # Sum the weighted memories together\n",
    "        # Input:  [batch Size, embedding size, <max length>]\n",
    "        # Output: [Batch Size, Embedding Size]\n",
    "        # Weighted output vector\n",
    "        # equation 3\n",
    "        weighted_output = neighborhood.sum(2)\n",
    "\n",
    "        return {'weight':attention, 'output':weighted_output}\n",
    "    \n",
    "    def forward(self, query, memory, output_memory, seq_length, maxlen=32):\n",
    "        # find maximum length of sequences in this batch\n",
    "        cur_max = torch.max(seq_length).item()\n",
    "        # slice to max length\n",
    "        memory = memory[:, :cur_max]\n",
    "        output_memory = output_memory[:, :cur_max]\n",
    "        \n",
    "        user_query, item_query = query\n",
    "        hop_outputs = []\n",
    "        \n",
    "        # hop 0\n",
    "        # z = m_u + e_i\n",
    "        z = user_query + item_query\n",
    "        \n",
    "        for hop_k in range(self.hops):\n",
    "            # hop 1, ... , hop self.hops-1\n",
    "            if hop_k != 0:                \n",
    "                # f(Wz + o + b)\n",
    "                # equation 6\n",
    "                z = F.relu(self.hop_mapping[hop_k](z) + memory_hop['output'])\n",
    "            \n",
    "            # apply attention\n",
    "            memory_hop = self.apply_attention_memory(memory, \n",
    "                                               output_memory,\n",
    "                                               z, \n",
    "                                               seq_length, \n",
    "                                               maxlen)\n",
    "            hop_outputs.append(memory_hop)\n",
    "        \n",
    "        return hop_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:29:05.978221Z",
     "start_time": "2019-07-09T13:29:05.967673Z"
    },
    "code_folding": [
     2,
     17
    ]
   },
   "outputs": [],
   "source": [
    "class OutputModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size):\n",
    "        super(OutputModule, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.dense = nn.Linear(self.embed_size*2, self.embed_size, bias=True)\n",
    "        self.dense.weight.requires_grad = True\n",
    "        self.dense.bias.requires_grad = True\n",
    "        nn.init.kaiming_normal_(self.dense.weight)\n",
    "        self.dense.bias.data.fill_(1.0)\n",
    "        \n",
    "        self.out = nn.Linear(self.embed_size, 1, bias = False)\n",
    "        self.out.weight.requires_grad = True\n",
    "        nn.init.xavier_uniform_(self.out.weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = F.relu(self.dense(inputs))\n",
    "        output = self.out(output)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:29:07.318880Z",
     "start_time": "2019-07-09T13:29:07.303749Z"
    },
    "code_folding": [
     0,
     2,
     25
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CollaborativeMemoryNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, user_embeddings, item_embeddings):\n",
    "        super(CollaborativeMemoryNetwork, self).__init__()\n",
    "\n",
    "        # MemoryEmbed\n",
    "        self.user_memory = nn.Embedding(user_embeddings.shape[0], user_embeddings.shape[1])\n",
    "        self.user_memory.weight = nn.Parameter(torch.from_numpy(user_embeddings))\n",
    "        self.user_memory.weight.requires_grad = True\n",
    "        \n",
    "        # ItemMemory\n",
    "        self.item_memory = nn.Embedding(item_embeddings.shape[0], item_embeddings.shape[1])\n",
    "        self.item_memory.weight = nn.Parameter(torch.from_numpy(item_embeddings))\n",
    "        self.item_memory.weight.requires_grad = True\n",
    "\n",
    "        # MemoryOutput\n",
    "        self.user_output = nn.Embedding(user_embeddings.shape[0], user_embeddings.shape[1])\n",
    "        # user_output is initialised with tf.truncated_normal_initializer(stddev=0.01)}\n",
    "        self.user_output.weight.requires_grad = True\n",
    "\n",
    "        self.mem_layer = VariableLengthMemoryLayer(2, config.embed_size)\n",
    "\n",
    "        self.output_module = OutputModule(config.embed_size)\n",
    "\n",
    "    \n",
    "    def forward(self, input_users, input_items, input_items_negative, \n",
    "                input_neighborhoods, input_neighborhood_lengths, \n",
    "                input_neighborhoods_negative, input_neighborhood_lengths_negative, evaluation=False):\n",
    "        \n",
    "        # get embeddings from user memory\n",
    "        cur_user = self.user_memory(input_users)\n",
    "        cur_user_output = self.user_output(input_users)\n",
    "\n",
    "        # get embeddings from item memory\n",
    "        cur_item = self.item_memory(input_items)\n",
    "        \n",
    "        # queries\n",
    "        query = (cur_user, cur_item)\n",
    "        \n",
    "        # positive\n",
    "        neighbor = self.mem_layer(query, \n",
    "                                  self.user_memory(input_neighborhoods), \n",
    "                                  self.user_output(input_neighborhoods), \n",
    "                                  input_neighborhood_lengths, \n",
    "                                  config.max_neighbors)[-1]['output']\n",
    "        \n",
    "        score = self.output_module(torch.cat((cur_user * cur_item, neighbor), 1))\n",
    "        \n",
    "        \n",
    "        if evaluation:\n",
    "            return score\n",
    "        \n",
    "        cur_item_negative = self.item_memory(input_items_negative)\n",
    "        neg_query = (cur_user, cur_item_negative)\n",
    "            \n",
    "        # negative\n",
    "        neighbor_negative = self.mem_layer(neg_query, \n",
    "                                           self.user_memory(input_neighborhoods_negative), \n",
    "                                           self.user_output(input_neighborhoods_negative), \n",
    "                                           input_neighborhood_lengths_negative, \n",
    "                                           config.max_neighbors)[-1]['output']\n",
    "        \n",
    "        negative_output = self.output_module(torch.cat((cur_user * cur_item_negative, \n",
    "                                                        neighbor_negative), 1))\n",
    "        \n",
    "        return score, negative_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:29:07.598890Z",
     "start_time": "2019-07-09T13:29:07.443372Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# loading pretrained embeddings\n",
    "embeddings = np.load(config.pretrain, allow_pickle=True)\n",
    "\n",
    "# initialize model\n",
    "model = CollaborativeMemoryNetwork(embeddings['user']*0.5, embeddings['item']*0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:29:08.387555Z",
     "start_time": "2019-07-09T13:29:08.364252Z"
    },
    "code_folding": [
     0,
     41,
     56
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def get_model_scores(test_data, neighborhood, max_neighbors, return_scores=False):\n",
    "    \"\"\"\n",
    "    test_data = dict([positive, np.array[negatives]])\n",
    "    \"\"\"\n",
    "    out = ''\n",
    "    scores = []\n",
    "    progress = tqdm(test_data.items(), total=len(test_data),\n",
    "                    leave=False, desc=u'Evaluate || ')\n",
    "    for user, (pos, neg) in progress:\n",
    "        item_indices = list(neg) + [pos]\n",
    "\n",
    "        input_users = torch.LongTensor([user] * (len(neg) + 1))\n",
    "        input_items = torch.LongTensor(item_indices)\n",
    "\n",
    "        if neighborhood is not None:\n",
    "            neighborhoods, neighborhood_length = (np.zeros((len(neg) + 1, max_neighbors), dtype=np.int32), \n",
    "                                                  np.ones(len(neg) + 1, dtype=np.int32))\n",
    "\n",
    "            for _idx, item in enumerate(item_indices):\n",
    "                _len = min(len(neighborhood.get(item, [])), max_neighbors)\n",
    "                if _len > 0:\n",
    "                    neighborhoods[_idx, :_len] = neighborhood[item][:_len]\n",
    "                    neighborhood_length[_idx] = _len\n",
    "                else:\n",
    "                    neighborhoods[_idx, :1] = user\n",
    "                    \n",
    "            input_neighborhoods = torch.LongTensor(neighborhoods)\n",
    "            input_neighborhood_lengths = torch.LongTensor(neighborhood_length)\n",
    "\n",
    "        score = model(input_users, input_items, None, input_neighborhoods, \n",
    "                      input_neighborhood_lengths, None, None, evaluation=True)\n",
    "        \n",
    "        scores.append(score.detach().numpy().ravel())\n",
    "        if return_scores:\n",
    "            s = ' '.join([\"{}:{}\".format(n, s) for s, n in zip(score.detach().numpy().ravel().tolist(), item_indices)])\n",
    "            out += \"{}\\t{}\\n\".format(user, s)\n",
    "    if return_scores:\n",
    "        return scores, out\n",
    "    return scores\n",
    "\n",
    "\n",
    "def evaluate_model(test_data, neighborhood, max_neighbors, EVAL_AT=[1, 5, 10]):\n",
    "    scores = get_model_scores(test_data, neighborhood, max_neighbors)\n",
    "    hrs = []\n",
    "    ndcgs = []\n",
    "    s = '\\n'\n",
    "    for k in EVAL_AT:\n",
    "        hr, ndcg = get_eval(scores, len(scores[0]) - 1, k)\n",
    "        s += \"{:<14} {:<14.6f}{:<14} {:.6f}\\n\".format('HR@%s' % k, hr, 'NDCG@%s' % k, ndcg)\n",
    "        hrs.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    print(s + '\\n')\n",
    "\n",
    "    return hrs, ndcgs\n",
    "\n",
    "\n",
    "def get_eval(scores, index, top_n=10):\n",
    "    \"\"\"\n",
    "    if the last element is the correct one, then\n",
    "    index = len(scores[0])-1\n",
    "    \"\"\"\n",
    "    ndcg = 0.0\n",
    "    hr = 0.0\n",
    "    assert len(scores[0]) > index and index >= 0\n",
    "\n",
    "    for score in scores:\n",
    "        # Get the top n indices\n",
    "        arg_index = np.argsort(-score)[:top_n]\n",
    "        if index in arg_index:\n",
    "            # Get the position\n",
    "            ndcg += np.log(2.0) / np.log(arg_index.tolist().index(index) + 2.0)\n",
    "            # Increment\n",
    "            hr += 1.0\n",
    "\n",
    "    return hr / len(scores), ndcg / len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:30:05.724443Z",
     "start_time": "2019-07-09T13:29:10.530963Z"
    },
    "code_folding": [
     7
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate, \n",
    "                                momentum=config.momentum)\n",
    "# scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=config.decay_rate)\n",
    "\n",
    "criterion = LossLayer()\n",
    "\n",
    "loss = []\n",
    "for i in range(config.epochs):    \n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # # Decay Learning Rate\n",
    "    # scheduler.step()\n",
    "    # # Print Learning Rate\n",
    "    # print('Epoch:', i,'LR:', scheduler.get_lr())\n",
    "    \n",
    "    progress = tqdm(enumerate(dataset.get_data(config.batch_size, True, config.neg_count)), \n",
    "                    dynamic_ncols=True, total=(dataset.train_size * config.neg_count) // config.batch_size)\n",
    "    \n",
    "    for k, batch in progress:\n",
    "        \n",
    "        ratings, pos_neighborhoods, pos_neighborhood_length, neg_neighborhoods, neg_neighborhood_length = batch\n",
    "        \n",
    "        input_users = torch.LongTensor(np.array(ratings[:, 0], dtype=np.int32))\n",
    "        input_items = torch.LongTensor(np.array(ratings[:, 1], dtype=np.int32))\n",
    "        input_items_negative = torch.LongTensor(np.array(ratings[:, 2], dtype=np.int32))\n",
    "        input_neighborhoods = torch.LongTensor(np.array(pos_neighborhoods, dtype=np.int32))\n",
    "        input_neighborhood_lengths = torch.LongTensor(np.array(pos_neighborhood_length, dtype=np.int32))\n",
    "        input_neighborhoods_negative = torch.LongTensor(np.array(neg_neighborhoods, dtype=np.int32))\n",
    "        input_neighborhood_lengths_negative = torch.LongTensor(np.array(neg_neighborhood_length, dtype=np.int32))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        score_pos, score_neg = model(input_users, input_items, input_items_negative, \n",
    "                                     input_neighborhoods, input_neighborhood_lengths, \n",
    "                                     input_neighborhoods_negative, input_neighborhood_lengths_negative)\n",
    "        \n",
    "        batch_loss = criterion(score_pos, score_neg)\n",
    "        \n",
    "        # adding l2 regularisation\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in ['mem_layer.hop_mapping.weight', \n",
    "                        'output_module.dense.weight', \n",
    "                        'output_module.out.weight']:\n",
    "                l2 = torch.sqrt(param.pow(2).sum())\n",
    "                batch_loss += (config.l2_lambda * l2)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        # plot_grad_flow(model.named_parameters())\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss.append(batch_loss.item())\n",
    "        progress.set_description(u\"[{}] Loss: {:,.4f} » » » » \".format(i, batch_loss.item()))\n",
    "    \n",
    "    print(\"Epoch {}: Avg Loss/Batch {:<20,.6f}\".format(i, np.mean(loss)))\n",
    "    model.eval()\n",
    "    evaluate_model(dataset.test_data, dataset.item_users_list, config.max_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:00:24.891269Z",
     "start_time": "2019-07-09T12:59:44.652060Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EVAL_AT = range(1, 11)\n",
    "hrs, ndcgs = [], []\n",
    "s = \"\"\n",
    "scores, out = get_model_scores(dataset.test_data, dataset.item_users_list, config.max_neighbors, True)\n",
    "\n",
    "for k in EVAL_AT:\n",
    "    hr, ndcg = get_eval(scores, len(scores[0])-1, k)\n",
    "    hrs.append(hr)\n",
    "    ndcgs.append(ndcg)\n",
    "    s += \"{:<14} {:<14.6f}{:<14} {:.6f}\\n\".format('HR@%s' % k, hr,\n",
    "                                                  'NDCG@%s' % k, ndcg)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:06:09.428125Z",
     "start_time": "2019-07-09T13:06:09.373988Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Saving training log...')\n",
    "with open(\"{}/{}\".format(config.logdir, config.version+'.log'), 'w') as fout:\n",
    "    header = ','.join([str(k) for k in EVAL_AT])\n",
    "    fout.write(\"{},{}\\n\".format('metric', header))\n",
    "    ndcg = ','.join([str(x) for x in ndcgs])\n",
    "    hr = ','.join([str(x) for x in hrs])\n",
    "    fout.write(\"ndcg,{}\\n\".format(ndcg))\n",
    "    fout.write(\"hr,{}\".format(hr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T13:05:32.733923Z",
     "start_time": "2019-07-09T13:05:32.714057Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model weights\n",
    "print(\"Saving model...\")\n",
    "torch.save(model.state_dict(), config.ssdir+config.version+'.ss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-09T08:55:21.748762Z",
     "start_time": "2019-07-09T08:55:21.623588Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model weights\n",
    "print('Loading model...')\n",
    "model = CollaborativeMemoryNetwork(embeddings['user']*0.5, embeddings['item']*0.5)\n",
    "model.load_state_dict(torch.load(config.logdir+config.version))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
