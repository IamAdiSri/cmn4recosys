{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "# Collaborative Memory Network for Recommendation Systems\n",
    "_**Ebesu, Shen, Fang** - The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval - SIGIR '18_\n",
    "\n",
    "\n",
    "This notebook by **Aditya Srivastava** is a PyTorch port to the TensorFlow code originally by the authors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:42.136661Z",
     "start_time": "2019-07-08T14:27:41.631334Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import functools\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:42.555222Z",
     "start_time": "2019-07-08T14:27:42.218220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:44.023781Z",
     "start_time": "2019-07-08T14:27:44.015470Z"
    }
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    resume = False\n",
    "    logdir = 'snapshots/'\n",
    "    version = 'model_0'\n",
    "    dataset = 'data/citeulike-a.npz'\n",
    "    pretrain = 'pretrain/citeulike-a_e50.npz'\n",
    "    embed_size = 50\n",
    "    epochs = 5 # training epochs (originally 30)\n",
    "    batch_size = 128\n",
    "    hops = 2 # number of hops/layers\n",
    "    l2_lambda = 0.1 # l2 regularization\n",
    "    neg_count = 4 # negative samples count\n",
    "    optimizer = 'rmsprop'\n",
    "    learning_rate = 0.001\n",
    "    decay_rate = 0.9\n",
    "    momentum = 0.9\n",
    "    grad_clip = 5.0\n",
    "    tol = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:45.478513Z",
     "start_time": "2019-07-08T14:27:45.459050Z"
    },
    "code_folding": [
     0
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-06T10:12:50.253791Z",
     "start_time": "2019-07-06T10:11:46.805Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model weights\n",
    "model = CollaborativeMemoryNetwork(embeddings['user']*0.5, embeddings['item']*0.5)\n",
    "model.load_state_dict(torch.load(config.logdir+config.version))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:47.525314Z",
     "start_time": "2019-07-08T14:27:47.483694Z"
    },
    "code_folding": [
     2,
     27,
     35,
     42,
     48,
     54,
     72,
     83
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        \"\"\"\n",
    "        Wraps dataset and produces batches for the model to consume\n",
    "\n",
    "        :param filename: path to training data for npz file\n",
    "        \"\"\"\n",
    "        self._data = np.load(filename, allow_pickle=True)\n",
    "        self.train_data = self._data['train_data'][:, :2]\n",
    "        self.test_data = self._data['test_data'].tolist()\n",
    "        self._train_index = np.arange(len(self.train_data), dtype=np.uint)\n",
    "        self._n_users, self._n_items = self.train_data.max(axis=0) + 1\n",
    "\n",
    "        # Neighborhoods\n",
    "        self.user_items = defaultdict(set)\n",
    "        self.item_users = defaultdict(set)\n",
    "        for u, i in self.train_data:\n",
    "            self.user_items[u].add(i)\n",
    "            self.item_users[i].add(u)\n",
    "        # Get a list version so we do not need to perform type casting\n",
    "        self.item_users_list = {k: list(v) for k, v in self.item_users.items()}\n",
    "        self._max_user_neighbors = max([len(x) for x in self.item_users.values()])\n",
    "        self.user_items = dict(self.user_items)\n",
    "        self.item_users = dict(self.item_users)\n",
    "\n",
    "    @property\n",
    "    def train_size(self):\n",
    "        \"\"\"\n",
    "        :return: number of examples in training set\n",
    "        :rtype: int\n",
    "        \"\"\"\n",
    "        return len(self.train_data)\n",
    "\n",
    "    @property\n",
    "    def user_count(self):\n",
    "        \"\"\"\n",
    "        Number of users in dataset\n",
    "        \"\"\"\n",
    "        return self._n_users\n",
    "\n",
    "    @property\n",
    "    def item_count(self):\n",
    "        \"\"\"\n",
    "        Number of items in dataset\n",
    "        \"\"\"\n",
    "        return self._n_items\n",
    "\n",
    "    def _sample_item(self):\n",
    "        \"\"\"\n",
    "        Draw an item uniformly\n",
    "        \"\"\"\n",
    "        return np.random.randint(0, self.item_count)\n",
    "\n",
    "    def _sample_negative_item(self, user_id):\n",
    "        \"\"\"\n",
    "        Uniformly sample a negative item\n",
    "        \"\"\"\n",
    "        if user_id > self.user_count:\n",
    "            raise ValueError(\"Trying to sample user id: {} > user count: {}\".format(\n",
    "                user_id, self.user_count))\n",
    "\n",
    "        n = self._sample_item()\n",
    "        positive_items = self.user_items[user_id]\n",
    "\n",
    "        if len(positive_items) >= self.item_count:\n",
    "            raise ValueError(\"The User has rated more items than possible %s / %s\" % (\n",
    "                len(positive_items), self.item_count))\n",
    "        while n in positive_items or n not in self.item_users:\n",
    "            n = self._sample_item()\n",
    "        return n\n",
    "\n",
    "    def _generate_data(self, neg_count):\n",
    "        idx = 0\n",
    "        self._examples = np.zeros((self.train_size*neg_count, 3),\n",
    "                                  dtype=np.uint32)\n",
    "        self._examples[:, :] = 0\n",
    "        for user_idx, item_idx in self.train_data:\n",
    "            for _ in range(neg_count):\n",
    "                neg_item_idx = self._sample_negative_item(user_idx)\n",
    "                self._examples[idx, :] = [user_idx, item_idx, neg_item_idx]\n",
    "                idx += 1\n",
    "\n",
    "    def get_data(self, batch_size: int, neighborhood: bool, neg_count: int):\n",
    "        \"\"\"\n",
    "        Batch data together as (user, item, negative item), pos_neighborhood,\n",
    "        length of neighborhood, negative_neighborhood, length of negative neighborhood\n",
    "\n",
    "        if neighborhood is False returns only user, item, negative_item so we\n",
    "        can reuse this for non-neighborhood-based methods.\n",
    "\n",
    "        :param batch_size: size of the batch\n",
    "        :param neighborhood: return the neighborhood information or not\n",
    "        :param neg_count: number of negative samples to uniformly draw per a pos\n",
    "                          example\n",
    "        :return: generator\n",
    "        \"\"\"\n",
    "        # Allocate inputs\n",
    "        batch = np.zeros((batch_size, 3), dtype=np.uint32)\n",
    "        pos_neighbor = np.zeros((batch_size, self._max_user_neighbors), dtype=np.int32)\n",
    "        pos_length = np.zeros(batch_size, dtype=np.int32)\n",
    "        neg_neighbor = np.zeros((batch_size, self._max_user_neighbors), dtype=np.int32)\n",
    "        neg_length = np.zeros(batch_size, dtype=np.int32)\n",
    "\n",
    "        # Shuffle index\n",
    "        np.random.shuffle(self._train_index)\n",
    "\n",
    "        idx = 0\n",
    "        for user_idx, item_idx in self.train_data[self._train_index]:\n",
    "            # TODO: set positive values outside of for loop\n",
    "            for _ in range(neg_count):\n",
    "                neg_item_idx = self._sample_negative_item(user_idx)\n",
    "                batch[idx, :] = [user_idx, item_idx, neg_item_idx]\n",
    "\n",
    "                # Get neighborhood information\n",
    "                if neighborhood:\n",
    "                    if len(self.item_users.get(item_idx, [])) > 0:\n",
    "                        pos_length[idx] = len(self.item_users[item_idx])\n",
    "                        pos_neighbor[idx, :pos_length[idx]] = self.item_users_list[item_idx]\n",
    "                    else:\n",
    "                        # Length defaults to 1\n",
    "                        pos_length[idx] = 1\n",
    "                        pos_neighbor[idx, 0] = item_idx\n",
    "\n",
    "                    if len(self.item_users.get(neg_item_idx, [])) > 0:\n",
    "                        neg_length[idx] = len(self.item_users[neg_item_idx])\n",
    "                        neg_neighbor[idx, :neg_length[idx]] = self.item_users_list[neg_item_idx]\n",
    "                    else:\n",
    "                        # Length defaults to 1\n",
    "                        neg_length[idx] = 1\n",
    "                        neg_neighbor[idx, 0] = neg_item_idx\n",
    "\n",
    "                idx += 1\n",
    "                # Yield batch if we filled queue\n",
    "                if idx == batch_size:\n",
    "                    if neighborhood:\n",
    "                        max_length = max(neg_length.max(), pos_length.max())\n",
    "                        yield batch, pos_neighbor[:, :max_length], pos_length, \\\n",
    "                              neg_neighbor[:, :max_length], neg_length\n",
    "                        pos_length[:] = 1\n",
    "                        neg_length[:] = 1\n",
    "                    else:\n",
    "                        yield batch\n",
    "                    # Reset\n",
    "                    idx = 0\n",
    "\n",
    "        # Provide remainder\n",
    "        if idx > 0:\n",
    "            if neighborhood:\n",
    "                max_length = max(neg_length[:idx].max(), pos_length[:idx].max())\n",
    "                yield batch[:idx], pos_neighbor[:idx, :max_length], pos_length[:idx], \\\n",
    "                      neg_neighbor[:idx, :max_length], neg_length[:idx]\n",
    "            else:\n",
    "                yield batch[:idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:48.724817Z",
     "start_time": "2019-07-08T14:27:48.178453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16980 5551 311\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(config.dataset)\n",
    "\n",
    "config.item_count = dataset.item_count\n",
    "config.user_count = dataset.user_count\n",
    "config.max_neighbors = dataset._max_user_neighbors\n",
    "\n",
    "print(dataset.item_count, dataset.user_count, dataset._max_user_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:49.867503Z",
     "start_time": "2019-07-08T14:27:49.856070Z"
    },
    "code_folding": [
     1,
     4,
     13
    ]
   },
   "outputs": [],
   "source": [
    "class LossLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossLayer, self).__init__()\n",
    "\n",
    "    def forward(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: predicted value\n",
    "        :param y: ground truth\n",
    "        :returns: Loss\n",
    "        \"\"\"        \n",
    "        bprl = torch.squeeze(self.bpr_loss(X, y))        \n",
    "        return bprl\n",
    "    \n",
    "    def bpr_loss(self, positive, negative):\n",
    "        r\"\"\"\n",
    "        Pairwise Loss from Bayesian Personalized Ranking.\n",
    "\n",
    "        \\log \\sigma(pos - neg)\n",
    "\n",
    "        where \\sigma is the sigmoid function, we try to set the ranking\n",
    "\n",
    "        if pos > neg = + number\n",
    "        if neg < pos = - number\n",
    "\n",
    "        Then applying the sigmoid to obtain a monotonically increasing function. Any\n",
    "        monotonically increasing function could be used, eg piecewise or probit.\n",
    "\n",
    "        :param positive: Score of prefered example\n",
    "        :param negative: Score of negative example\n",
    "        :param name: str, name scope\n",
    "        :returns: mean loss\n",
    "        \"\"\"\n",
    "        difference = positive - negative\n",
    "        # Numerical stability\n",
    "        eps = 1e-12\n",
    "        loss = -1*torch.log(torch.sigmoid(difference) + eps)\n",
    "        return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:51.567824Z",
     "start_time": "2019-07-08T14:27:51.536621Z"
    },
    "code_folding": [
     1,
     13,
     50,
     109
    ]
   },
   "outputs": [],
   "source": [
    "class VariableLengthMemoryLayer(nn.Module):\n",
    "    def __init__(self, hops, embed_size):\n",
    "        super(VariableLengthMemoryLayer, self).__init__()\n",
    "        \n",
    "        self.hops = hops\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.hop_mapping = nn.Linear(self.embed_size, self.embed_size, bias=True)\n",
    "        self.hop_mapping.weight.requires_grad = True\n",
    "        self.hop_mapping.bias.requires_grad = True\n",
    "        nn.init.kaiming_normal(self.hop_mapping.weight)\n",
    "        self.hop_mapping.bias.data.fill_(1.0)\n",
    "    \n",
    "    def mask_mod(self, inputs, mask_length, maxlen=None):\n",
    "        \"\"\"\n",
    "        Apply a memory mask such that the values we mask result in being the\n",
    "        minimum possible value we can represent with a float32.\n",
    "\n",
    "        :param inputs: [batch size, length], dtype=tf.float32\n",
    "        :param memory_mask: [batch_size] shape Tensor of ints indicating the\n",
    "            length of inputs\n",
    "        :param maxlen: Sets the maximum length of the sequence; if None infered\n",
    "            from inputs\n",
    "        :returns: [batch size, length] dim Tensor with the mask applied\n",
    "        \"\"\"\n",
    "        # [batch_size, length] => Sequence Mask\n",
    "        memory_mask = torch.arange(maxlen).expand(len(mask_length), maxlen) < mask_length.unsqueeze(1)\n",
    "        memory_mask = memory_mask.float()\n",
    "\n",
    "        num_remaining_memory_slots = torch.sum(memory_mask, 1)\n",
    "\n",
    "        # Get the numerical limits of a float\n",
    "        finfo = np.finfo(np.float32)\n",
    "        # print(finfo)\n",
    "\n",
    "        # If True = 1 = Keep that memory slot\n",
    "        kept_indices = memory_mask\n",
    "\n",
    "        # Inverse\n",
    "        ignored_indices = memory_mask < 1\n",
    "        ignored_indices = ignored_indices.float()\n",
    "\n",
    "        # If we keep the indices its the max float value else its the\n",
    "        # minimum float value. Then we can take the minimum\n",
    "        lower_bound = finfo.max * kept_indices + finfo.min * ignored_indices\n",
    "        slice_length = torch.max(mask_length)\n",
    "        \n",
    "        # Return the elementwise\n",
    "        return torch.min(inputs[:, :slice_length], lower_bound[:, :slice_length])\n",
    "        \n",
    "    def apply_attention_memory(self, memory, output_memory, query, memory_mask=None, maxlen=None):\n",
    "        \"\"\"\n",
    "            :param memory: [batch size, max length, embedding size],\n",
    "                typically Matrix M\n",
    "            :param output_memory: [batch size, max length, embedding size],\n",
    "                typically Matrix C\n",
    "            :param query: [batch size, embed size], typically u\n",
    "            :param memory_mask: [batch size] dim Tensor, the length of each\n",
    "                sequence if variable length\n",
    "            :param maxlen: int/Tensor, the maximum sequence padding length; if None it\n",
    "                infers based on the max of memory_mask\n",
    "            :returns: AttentionOutput\n",
    "                 output: [batch size, embedding size]\n",
    "                 weight: [batch size, max length], the attention weights applied to\n",
    "                         the output representation.\n",
    "        \"\"\"\n",
    "        # query = [batch size, embeddings] => expand => [batch size, embeddings, 1]\n",
    "        # transpose => [batch size, 1, embeddings]\n",
    "        query_expanded = query.unsqueeze(-1).transpose(2, 1)\n",
    "\n",
    "        # Apply batched dot product\n",
    "        # memory = [batch size, <Max Length>, Embeddings]\n",
    "        # Broadcast the same memory across each dimension of max length\n",
    "        # We obtain an attention value for each memory,\n",
    "        # ie a_0 p_0, a_1 p_1, .. a_n p_n, which equates to the max length\n",
    "        #    because our query is only 1 dim, we only get attention over memory\n",
    "        #    for that query. If our query was 2-d then we would obtain a matrix.\n",
    "        # Return: [batch size, max length]\n",
    "        batched_dot_prod = query_expanded * memory\n",
    "        scores = batched_dot_prod.sum(2)\n",
    "\n",
    "        if memory_mask is not None:\n",
    "            scores = self.mask_mod(scores, memory_mask, maxlen)\n",
    "\n",
    "        # Attention over memories: [Batch Size, <Max Length>]\n",
    "        # paper equation 2\n",
    "        attention = F.softmax(scores)\n",
    "\n",
    "        # [Batch Size, <Max Length>] => [Batch Size, 1, <Max Length>]\n",
    "        probs_temp = attention.unsqueeze(1)\n",
    "\n",
    "        # Output_Memories = [batch size, <Max Length>, Embeddings]\n",
    "        # Transpose = [Batch Size, Embedding Size, <Max Length>]\n",
    "        c_temp = output_memory.transpose(2, 1)\n",
    "\n",
    "        # Apply a weighted scalar or attention to the external memory \n",
    "        # to get weighted neighborhood\n",
    "        # [batch size, 1, <max length>] * [batch size, embedding size, <max length>]\n",
    "        neighborhood = c_temp * probs_temp\n",
    "\n",
    "        # Sum the weighted memories together\n",
    "        # Input:  [batch Size, embedding size, <max length>]\n",
    "        # Output: [Batch Size, Embedding Size]\n",
    "        # Weighted output vector\n",
    "        # paper equation 3\n",
    "        weighted_output = neighborhood.sum(2)\n",
    "\n",
    "        return {'weight':attention, 'output':weighted_output}\n",
    "    \n",
    "    def forward(self, query, memory, output_memory, seq_length, maxlen=32):\n",
    "        # find maximum length of sequences in this batch\n",
    "        cur_max = torch.max(seq_length).item()\n",
    "        # slice to max length\n",
    "        memory = memory[:, :cur_max]\n",
    "        output_memory = output_memory[:, :cur_max]\n",
    "        \n",
    "        user_query, item_query = query\n",
    "        hop_outputs = []\n",
    "        \n",
    "        # hop 0\n",
    "        # z = m_u + e_i\n",
    "        z = user_query + item_query\n",
    "        \n",
    "        for hop_k in range(self.hops):\n",
    "            # hop 1, ... , hop self.hops-1\n",
    "            if hop_k != 0:                \n",
    "                # f(Wz + o + b)\n",
    "                # equation 6\n",
    "                z = F.relu(self.hop_mapping(z) + memory_hop['output'])\n",
    "            \n",
    "            # apply attention\n",
    "            memory_hop = self.apply_attention_memory(memory, \n",
    "                                               output_memory,\n",
    "                                               z, \n",
    "                                               seq_length, \n",
    "                                               maxlen)\n",
    "            hop_outputs.append(memory_hop)\n",
    "        \n",
    "        return hop_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:52.491815Z",
     "start_time": "2019-07-08T14:27:52.480464Z"
    },
    "code_folding": [
     2,
     17
    ]
   },
   "outputs": [],
   "source": [
    "class OutputModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_size):\n",
    "        super(OutputModule, self).__init__()\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        self.dense = nn.Linear(self.embed_size*2, self.embed_size, bias=True)\n",
    "        self.dense.weight.requires_grad = True\n",
    "        self.dense.bias.requires_grad = True\n",
    "        nn.init.kaiming_normal_(self.dense.weight)\n",
    "        self.dense.bias.data.fill_(1.0)\n",
    "        \n",
    "        self.out = nn.Linear(self.embed_size, 1, bias = False)\n",
    "        self.out.weight.requires_grad = True\n",
    "        nn.init.xavier_uniform_(self.out.weight)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        output = F.relu(self.dense(inputs))\n",
    "        output = self.out(output)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:53.307226Z",
     "start_time": "2019-07-08T14:27:53.287541Z"
    },
    "code_folding": [
     2,
     25
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CollaborativeMemoryNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, user_embeddings, item_embeddings):\n",
    "        super(CollaborativeMemoryNetwork, self).__init__()\n",
    "\n",
    "        # MemoryEmbed\n",
    "        self.user_memory = nn.Embedding(user_embeddings.shape[0], user_embeddings.shape[1])\n",
    "        self.user_memory.weight = nn.Parameter(torch.from_numpy(user_embeddings))\n",
    "        self.user_memory.weight.requires_grad = True\n",
    "        \n",
    "        # ItemMemory\n",
    "        self.item_memory = nn.Embedding(item_embeddings.shape[0], item_embeddings.shape[1])\n",
    "        self.item_memory.weight = nn.Parameter(torch.from_numpy(item_embeddings))\n",
    "        self.item_memory.weight.requires_grad = True\n",
    "\n",
    "        # MemoryOutput\n",
    "        self.user_output = nn.Embedding(user_embeddings.shape[0], user_embeddings.shape[1])\n",
    "        # user_output is initialised with tf.truncated_normal_initializer(stddev=0.01)}\n",
    "        self.user_output.weight.requires_grad = True\n",
    "\n",
    "        self.mem_layer = VariableLengthMemoryLayer(2, config.embed_size)\n",
    "\n",
    "        self.output_module = OutputModule(config.embed_size)\n",
    "\n",
    "    \n",
    "    def forward(self, input_users, input_items, input_items_negative, \n",
    "                input_neighborhoods, input_neighborhood_lengths, \n",
    "                input_neighborhoods_negative, input_neighborhood_lengths_negative, evaluation=False):\n",
    "        \n",
    "        # get embeddings from user memory\n",
    "        cur_user = self.user_memory(input_users)\n",
    "        cur_user_output = self.user_output(input_users)\n",
    "\n",
    "        # get embeddings from item memory\n",
    "        cur_item = self.item_memory(input_items)\n",
    "        \n",
    "        # queries\n",
    "        query = (cur_user, cur_item)\n",
    "        \n",
    "        # positive\n",
    "        neighbor = self.mem_layer(query, \n",
    "                                  self.user_memory(input_neighborhoods), \n",
    "                                  self.user_output(input_neighborhoods), \n",
    "                                  input_neighborhood_lengths, \n",
    "                                  config.max_neighbors)[-1]['output']\n",
    "        \n",
    "        score = self.output_module(torch.cat((cur_user * cur_item, neighbor), 1))\n",
    "        \n",
    "        \n",
    "        if evaluation:\n",
    "            return score\n",
    "        \n",
    "        cur_item_negative = self.item_memory(input_items_negative)\n",
    "        neg_query = (cur_user, cur_item_negative)\n",
    "            \n",
    "        # negative\n",
    "        neighbor_negative = self.mem_layer(neg_query, \n",
    "                                           self.user_memory(input_neighborhoods_negative), \n",
    "                                           self.user_output(input_neighborhoods_negative), \n",
    "                                           input_neighborhood_lengths_negative, \n",
    "                                           config.max_neighbors)[-1]['output']\n",
    "        \n",
    "        negative_output = self.output_module(torch.cat((cur_user * cur_item_negative, \n",
    "                                                        neighbor_negative), 1))\n",
    "        \n",
    "        return score, negative_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:54.330133Z",
     "start_time": "2019-07-08T14:27:54.215992Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# loading pretrained embeddings\n",
    "embeddings = np.load(config.pretrain, allow_pickle=True)\n",
    "\n",
    "# initialize model\n",
    "model = CollaborativeMemoryNetwork(embeddings['user']*0.5, embeddings['item']*0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T14:27:55.808355Z",
     "start_time": "2019-07-08T14:27:55.780244Z"
    },
    "code_folding": [
     0,
     41,
     56
    ],
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "def get_model_scores(test_data, neighborhood, max_neighbors, return_scores=False):\n",
    "    \"\"\"\n",
    "    test_data = dict([positive, np.array[negatives]])\n",
    "    \"\"\"\n",
    "    out = ''\n",
    "    scores = []\n",
    "    progress = tqdm(test_data.items(), total=len(test_data),\n",
    "                    leave=False, desc=u'Evaluate || ')\n",
    "    for user, (pos, neg) in progress:\n",
    "        item_indices = list(neg) + [pos]\n",
    "\n",
    "        input_users = torch.LongTensor([user] * (len(neg) + 1))\n",
    "        input_items = torch.LongTensor(item_indices)\n",
    "\n",
    "        if neighborhood is not None:\n",
    "            neighborhoods, neighborhood_length = (np.zeros((len(neg) + 1, max_neighbors), dtype=np.int32), \n",
    "                                                  np.ones(len(neg) + 1, dtype=np.int32))\n",
    "\n",
    "            for _idx, item in enumerate(item_indices):\n",
    "                _len = min(len(neighborhood.get(item, [])), max_neighbors)\n",
    "                if _len > 0:\n",
    "                    neighborhoods[_idx, :_len] = neighborhood[item][:_len]\n",
    "                    neighborhood_length[_idx] = _len\n",
    "                else:\n",
    "                    neighborhoods[_idx, :1] = user\n",
    "                    \n",
    "            input_neighborhoods = torch.LongTensor(neighborhoods)\n",
    "            input_neighborhood_lengths = torch.LongTensor(neighborhood_length)\n",
    "\n",
    "        score = model(input_users, input_items, None, input_neighborhoods, \n",
    "                      input_neighborhood_lengths, None, None, evaluation=True)\n",
    "        \n",
    "        scores.append(score.detach().numpy().ravel())\n",
    "        if return_scores:\n",
    "            s = ' '.join([\"{}:{}\".format(n, s) for s, n in zip(score.ravel().tolist(), item_indices)])\n",
    "            out += \"{}\\t{}\\n\".format(user, s)\n",
    "    if return_scores:\n",
    "        return scores, out\n",
    "    return scores\n",
    "\n",
    "\n",
    "def evaluate_model(test_data, neighborhood, max_neighbors, EVAL_AT=[1, 5, 10]):\n",
    "    scores = get_model_scores(test_data, neighborhood, max_neighbors)\n",
    "    hrs = []\n",
    "    ndcgs = []\n",
    "    s = '\\n'\n",
    "    for k in EVAL_AT:\n",
    "        hr, ndcg = get_eval(scores, len(scores[0]) - 1, k)\n",
    "        s += \"{:<14} {:<14.6f}{:<14} {:.6f}\\n\".format('HR@%s' % k, hr, 'NDCG@%s' % k, ndcg)\n",
    "        hrs.append(hr)\n",
    "        ndcgs.append(ndcg)\n",
    "    print(s + '\\n')\n",
    "\n",
    "    return hrs, ndcgs\n",
    "\n",
    "\n",
    "def get_eval(scores, index, top_n=10):\n",
    "    \"\"\"\n",
    "    if the last element is the correct one, then\n",
    "    index = len(scores[0])-1\n",
    "    \"\"\"\n",
    "    ndcg = 0.0\n",
    "    hr = 0.0\n",
    "    assert len(scores[0]) > index and index >= 0\n",
    "\n",
    "    for score in scores:\n",
    "        # Get the top n indices\n",
    "        arg_index = np.argsort(-score)[:top_n]\n",
    "        if index in arg_index:\n",
    "            # Get the position\n",
    "            ndcg += np.log(2.0) / np.log(arg_index.tolist().index(index) + 2.0)\n",
    "            # Increment\n",
    "            hr += 1.0\n",
    "\n",
    "    return hr / len(scores), ndcg / len(scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-08T16:45:19.075590Z",
     "start_time": "2019-07-08T15:51:11.431330Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6bdde1b15d45599500e43bd87d44c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=6232), HTML(value='')), layout=Layout(display…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0: Avg Loss/Batch 0.157849            \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluate || ', max=5551, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HR@1           0.396145      NDCG@1         0.396145\n",
      "HR@5           0.761845      NDCG@5         0.591336\n",
      "HR@10          0.871375      NDCG@10        0.627073\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e68d6fe20b43dc82c8340bcdcf93fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=6232), HTML(value='')), layout=Layout(display…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Avg Loss/Batch 0.153894            \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluate || ', max=5551, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HR@1           0.393082      NDCG@1         0.393082\n",
      "HR@5           0.754098      NDCG@5         0.586729\n",
      "HR@10          0.865610      NDCG@10        0.623103\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73a88ca9545490e8594d7157061ec3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=6232), HTML(value='')), layout=Layout(display…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Avg Loss/Batch 0.151464            \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluate || ', max=5551, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HR@1           0.398126      NDCG@1         0.398126\n",
      "HR@5           0.770492      NDCG@5         0.597167\n",
      "HR@10          0.874257      NDCG@10        0.630921\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577ead1ac44d4478adec5e30be55c48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=6232), HTML(value='')), layout=Layout(display…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Avg Loss/Batch 0.148590            \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluate || ', max=5551, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HR@1           0.399207      NDCG@1         0.399207\n",
      "HR@5           0.767429      NDCG@5         0.597300\n",
      "HR@10          0.871375      NDCG@10        0.631181\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f6928887d742ac8a86bed38ec01054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, layout=Layout(flex='2'), max=6232), HTML(value='')), layout=Layout(display…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5f4ca60a856c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mbatch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2_lambda\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#         plot_grad_flow(model.named_parameters())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Envs/dl/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.Envs/dl/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: write the save/resume method\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=config.learning_rate, \n",
    "                                momentum=config.momentum)\n",
    "\n",
    "criterion = LossLayer()\n",
    "\n",
    "loss = []\n",
    "for i in range(config.epochs):\n",
    "    model.train()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    progress = tqdm(enumerate(dataset.get_data(config.batch_size, True, config.neg_count)), \n",
    "                    dynamic_ncols=True, total=(dataset.train_size * config.neg_count) // config.batch_size)\n",
    "    \n",
    "    for k, batch in progress:\n",
    "#         with open('data/sample.pkl', 'rb') as inp:\n",
    "#             batch = pickle.load(inp)\n",
    "#             print(\"LOADED SAMPLE BATCH\")\n",
    "        \n",
    "        ratings, pos_neighborhoods, pos_neighborhood_length, neg_neighborhoods, neg_neighborhood_length = batch\n",
    "        \n",
    "        input_users = torch.LongTensor(np.array(ratings[:, 0], dtype=np.int32))\n",
    "        input_items = torch.LongTensor(np.array(ratings[:, 1], dtype=np.int32))\n",
    "        input_items_negative = torch.LongTensor(np.array(ratings[:, 2], dtype=np.int32))\n",
    "        input_neighborhoods = torch.LongTensor(np.array(pos_neighborhoods, dtype=np.int32))\n",
    "        input_neighborhood_lengths = torch.LongTensor(np.array(pos_neighborhood_length, dtype=np.int32))\n",
    "        input_neighborhoods_negative = torch.LongTensor(np.array(neg_neighborhoods, dtype=np.int32))\n",
    "        input_neighborhood_lengths_negative = torch.LongTensor(np.array(neg_neighborhood_length, dtype=np.int32))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        score_pos, score_neg = model(input_users, input_items, input_items_negative, \n",
    "                                     input_neighborhoods, input_neighborhood_lengths, \n",
    "                                     input_neighborhoods_negative, input_neighborhood_lengths_negative)\n",
    "        \n",
    "        batch_loss = criterion(score_pos, score_neg)\n",
    "        \n",
    "        # adding l2 regularisation\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in ['mem_layer.hop_mapping.weight', \n",
    "                        'output_module.dense.weight', \n",
    "                        'output_module.out.weight']:\n",
    "                l2 = torch.sqrt(param.pow(2).sum())\n",
    "                batch_loss += (config.l2_lambda * l2)\n",
    "\n",
    "        batch_loss.backward()\n",
    "        \n",
    "#         plot_grad_flow(model.named_parameters())\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        loss.append(batch_loss.item())\n",
    "        progress.set_description(u\"[{}] Loss: {:,.4f} » » » » \".format(i, batch_loss.item()))\n",
    "#         break\n",
    "        \n",
    "    print(\"Epoch {}: Avg Loss/Batch {:<20,.6f}\".format(i, np.mean(loss)))\n",
    "    model.eval()\n",
    "    evaluate_model(dataset.test_data, dataset.item_users_list, config.max_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-07T22:12:43.598873Z",
     "start_time": "2019-07-07T22:09:58.077Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model weights\n",
    "torch.save(model.state_dict(), config.logdir+config.version)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
